import numpy as np

class Activation():
    """
    Template class to implement an activation function. Each child class must implement two methods, `forward` and `backward`.

    `forward` must accept an input, an np.array of shape (num_features,) or (num_samples, num_features) and return the 
    activated results, an np.array of shape matching the input.

    `backward` must accept the output generated by the forward function, and use it to compute the gradient of the activation
    output wrt the input to the activation function.
    """
    def __init__(self) -> None:
        pass

    def forward(self, x: np.array) -> np.array:
        """
        Returns the output of the activation function for input x.

        Arg: x (np.array): The input
        Returns: activation(x) (np.array): The output
        """
        return x

    def backward(self, output: np.array) -> np.array:
        """
        Returns the gradient of the output of the activation function wrt the corresponding inputs.

        Arg: output (np.array): The output produced by the activation function, corresponding to the input for which gradient is
            to be computed.
        Returns: grad (np.array): The gradient of the output of the activation function wrt the inputs (corresponding to the
            output which is passed as a proxy)
        """
        return output

class Identity(Activation):
    """
    Implements the identity function, f(x) = x
    """
    def __init__(self) -> None:
        super().__init__()

    def forward(self, x: np.array) -> np.array:
        """
        Returns identity(x) for input x

        Arg: x (np.array): The input
        Returns: identity(x) (np.array): The output, identity(x) (= x) 
        """
        return x
    
    def backward(self, output: np.array) -> np.array:
        """
        Returns gradient of identity output wrt inputs, given the output

        Arg: output (np.array): The output produced by the identity activation, corresponding to the input for which gradient
            is being computed.
        
        Returns:
            grad (np.array): The gradient of the identity activation wrt the required inputs (for which the output is passed
                as proxy)
        """
        return np.ones_like(output)

class Sigmoid(Activation):
    """
    Implements the sigmoid logistic function, f(x) = 1/(1 + exp(-x))
    """
    def __init__(self) -> None:
        super().__init__()

    def forward(self, x: np.array) -> np.array:
        """
        Returns sigmoid(x) for input x

        Arg: x (np.array): The input
        Returns: sigmoid(x) (np.array): The output, sigmoid(x)
        """
        return 1 / (1 + np.exp(-x))
    
    def backward(self, output: np.array) -> np.array:
        """
        Returns gradient of sigmoid outputs wrt inputs, given the output

        Arg: output (np.array): The output produced by the sigmoid activation, corresponding to the input for which gradient
            is being computed.
        
        Returns:
            grad (np.array): The gradient of the sigmoid activation wrt the required inputs (for which the output is passed
                as proxy)
        """
        return output * (1 - output)

class ReLU(Activation):
    """
    Implements the rectified linear unit (ReLU) function, f(x) = max(0, x) 
    """
    def __init__(self) -> None:
        super().__init__()
        
    def forward(self, x: np.array) -> np.array:
        """
        Returns relu(x) for input x

        Arg: x (np.array): The input
        Returns: relu(x) (np.array): The output, relu(x)
        """
        return np.maximum(0., x)
    
    def backward(self, output: np.array) -> np.array:
        """
        Returns gradient of relu outputs wrt inputs, given the output

        Arg: output (np.array): The output produced by the relu activation, corresponding to the input for which gradient
            is being computed.
        
        Returns:
            grad (np.array): The gradient of the relu activation wrt the required inputs (for which the output is passed
                as proxy)
        """
        return np.sign(output, dtype=np.float64)

class Tanh(Activation):
    """
    Implements the hyperbolic tan (tanh) function, f(x) = (exp(x) - exp(-x))/(exp(x) + exp(-x))
    """
    def __init__(self) -> None:
        super().__init__()

    def forward(self, x: np.array) -> np.array:
        """
        Returns tanh(x) for input x

        Arg: x (np.array): The input
        Returns: tanh(x) (np.array): The output, tanh(x)
        """
        # tanh(x) = (exp(2x) - 1)/(exp(2x) + 1) = 2 / (1 + exp(-2x)) - 1
        return 2 / (1 + np.exp(-2*x)) - 1
    
    def backward(self, output: np.array) -> np.array:
        """
        Returns gradient of tanh outputs wrt inputs, given the output

        Arg: output (np.array): The output produced by the tanh activation, corresponding to the input for which gradient
            is being computed.
        
        Returns:
            grad (np.array): The gradient of the tanh activation wrt the required inputs (for which the output is passed
                as proxy)
        """
        # d(tanh(x))/dx = 4exp(2x)/(1 + exp(2x))**2
        # exp(2x)/(1 + exp(2x)) = (tanh(x) + 1)/2, 1/(1 + exp(2x)) = (1 - tanh(x))/2
        # d(tanh(x))/dx = 1 - tanh(x)**2
        return 1 - output**2

class Softmax(Activation):
    """
    Implements the softmax function, f(x) = exp(x) / sum(exp(x))

    Arg: eps (float, optional): The minimum probability/output to produce. Provides stability in loss/gradient calculation.
        Defaults to 1e-8.
    """
    def __init__(self, eps: float=1e-8) -> None:
        super().__init__()
        self.eps = eps

    def forward(self, x: np.array) -> np.array:
        """
        Returns softmax(x) for input x

        Arg: x (np.array): The input
        Returns: softmax(x) (np.array): The output, softmax(x)
        """
        max_x = np.max(x, axis=-1).reshape(-1, 1)
        exp_x = np.exp(x - max_x) # to ensure overflow doesn't occur since this prevents any value from exceeding 1
        total = np.sum(exp_x, axis=-1).reshape(-1, 1)
        return np.maximum((exp_x / total).reshape(x.shape), self.eps) 
        # ensuring no output is lesser than eps, for numerical stability in cross-entropy loss/grad computation
    
    def backward(self, output: np.array) -> np.array: 
        """
        Returns gradient of softmax outputs wrt inputs, given the output and true class

        Arg: output (np.array): The output produced by the softmax activation, corresponding to the input for which gradient
            is being computed.
        
        Returns:
            grad (np.array): The gradient of the softmax activation wrt the required inputs (for which the output is passed
                as proxy)
        """
        # Softmax has a 2d gradient, corresponding to each output-input pair, for a single datapoint, and a vector of such
        # 2d gradients for multiple datapoints
        if len(output.shape) == 1:
            return output * np.eye(len(output)) - np.matmul(output.reshape(-1, 1), output.reshape(1, -1))
        else:
            return np.array([op * np.eye(len(op)) - np.matmul(op.reshape(-1, 1), op.reshape(1, -1)) for op in output])
        
