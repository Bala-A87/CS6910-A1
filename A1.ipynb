{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data found. Loading...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "data_dir = Path('./data/')\n",
    "data_path = Path('./data/fashion_mnist.npz')\n",
    "\n",
    "if data_path.is_file():\n",
    "    print('Data found. Loading...')\n",
    "    data = np.load(data_path)\n",
    "    X_train, Y_train, X_test, Y_test = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "else:\n",
    "    print('Data not found. Downloading...')\n",
    "    if not data_dir.is_dir():\n",
    "        os.mkdir(data_dir)\n",
    "    from keras.datasets import fashion_mnist\n",
    "    (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "    X_train, X_test = X_train/255., X_test/255.\n",
    "    np.savez_compressed(data_path, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    row_no = i//5\n",
    "    col_no = i%5\n",
    "    X_sub = X_train[Y_train==i]\n",
    "    index = np.random.choice(a=range(len(X_sub)), size=1)\n",
    "    ax[row_no][col_no].imshow(X_sub[index].reshape((28, 28)), cmap='gray')\n",
    "    ax[row_no][col_no].set_title(CLASS_LABELS[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity():\n",
    "    \"\"\"\n",
    "    Implements the identity function, f(x) = x\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns identity(x) for input x\n",
    "\n",
    "        Arg: x (np.array): The input\n",
    "        Returns: identity(x) (np.array): The output, identity(x) (= x) \n",
    "        \"\"\"\n",
    "        return x\n",
    "    \n",
    "    def backward(self, output: np.array, y_true=None) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns gradient of identity output wrt inputs, given the output\n",
    "\n",
    "        Args:\n",
    "            output (np.array): The output produced by the identity activation, corresponding to the input for which gradient\n",
    "                is being computed.\n",
    "            y_true (optional): Compatibility feature, required for softmax activation. Not used in computation.\n",
    "                Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            grad (np.array): The gradient of the identity activation wrt the required inputs (for which the output is passed\n",
    "                as proxy)\n",
    "        \"\"\"\n",
    "        return np.ones_like(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    \"\"\"\n",
    "    Implements the sigmoid logistic function, f(x) = 1/(1 + exp(-x))\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns sigmoid(x) for input x\n",
    "\n",
    "        Arg: x (np.array): The input\n",
    "        Returns: sigmoid(x) (np.array): The output, sigmoid(x)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, output: np.array, y_true=None) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns gradient of sigmoid outputs wrt inputs, given the output\n",
    "\n",
    "        Args:\n",
    "            output (np.array): The output produced by the sigmoid activation, corresponding to the input for which gradient\n",
    "                is being computed.\n",
    "            y_true (optional): Compatibility feature, required for softmax activation. Not used in computation.\n",
    "                Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            grad (np.array): The gradient of the sigmoid activation wrt the required inputs (for which the output is passed\n",
    "                as proxy)\n",
    "        \"\"\"\n",
    "        return output * (1 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    \"\"\"\n",
    "    Implements the rectified linear unit (ReLU) function, f(x) = max(0, x) \n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns relu(x) for input x\n",
    "\n",
    "        Arg: x (np.array): The input\n",
    "        Returns: relu(x) (np.array): The output, relu(x)\n",
    "        \"\"\"\n",
    "        return np.maximum(0., x)\n",
    "    \n",
    "    def backward(self, output: np.array, y_true=None) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns gradient of relu outputs wrt inputs, given the output\n",
    "\n",
    "        Args:\n",
    "            output (np.array): The output produced by the relu activation, corresponding to the input for which gradient\n",
    "                is being computed.\n",
    "            y_true (optional): Compatibility feature, required for softmax activation. Not used in computation.\n",
    "                Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            grad (np.array): The gradient of the relu activation wrt the required inputs (for which the output is passed\n",
    "                as proxy)\n",
    "        \"\"\"\n",
    "        return np.sign(output, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \"\"\"\n",
    "    Implements the hyperbolic tan (tanh) function, f(x) = (exp(x) - exp(-x))/(exp(x) + exp(-x))\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns tanh(x) for input x\n",
    "\n",
    "        Arg: x (np.array): The input\n",
    "        Returns: tanh(x) (np.array): The output, tanh(x)\n",
    "        \"\"\"\n",
    "        # tanh(x) = (exp(2x) - 1)/(exp(2x) + 1) = 2 / (1 + exp(-2x)) - 1\n",
    "        return 2 / (1 + np.exp(-2*x)) - 1\n",
    "    \n",
    "    def backward(self, output: np.array, y_true=None) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns gradient of tanh outputs wrt inputs, given the output\n",
    "\n",
    "        Args:\n",
    "            output (np.array): The output produced by the tanh activation, corresponding to the input for which gradient\n",
    "                is being computed.\n",
    "            y_true (optional): Compatibility feature, required for softmax activation. Not used in computation.\n",
    "                Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            grad (np.array): The gradient of the tanh activation wrt the required inputs (for which the output is passed\n",
    "                as proxy)\n",
    "        \"\"\"\n",
    "        # d(tanh(x))/dx = 4exp(2x)/(1 + exp(2x))**2\n",
    "        # exp(2x)/(1 + exp(2x)) = (tanh(x) + 1)/2, 1/(1 + exp(2x)) = (1 - tanh(x))/2\n",
    "        # d(tanh(x))/dx = 1 - tanh(x)**2\n",
    "        return 1 - output**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Implements the softmax function, f(x) = exp(x) / sum(exp(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float=1e-8) -> None:\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns softmax(x) for input x\n",
    "\n",
    "        Arg: x (np.array): The input\n",
    "        Returns: softmax(x) (np.array): The output, softmax(x)\n",
    "        \"\"\"\n",
    "        max_x = np.max(x, axis=-1).reshape(-1, 1)\n",
    "        exp_x = np.exp(x - max_x) + self.eps\n",
    "        total = np.sum(exp_x, axis=-1).reshape(-1, 1)\n",
    "        # return np.maximum((exp_x / total).reshape(x.shape), self.eps)\n",
    "        return (exp_x / total).reshape(x.shape)\n",
    "    \n",
    "    def backward(self, output: np.array, y_true) -> np.array: \n",
    "        \"\"\"\n",
    "        Returns gradient of softmax outputs wrt inputs, given the output and true class\n",
    "\n",
    "        Args:\n",
    "            output (np.array): The output produced by the softmax activation, corresponding to the input for which gradient\n",
    "                is being computed.\n",
    "            y_true : The true classes corresponding to the producing outputs. Could be a scalar or an array.\n",
    "        \n",
    "        Returns:\n",
    "            grad (np.array): The gradient of the softmax activation wrt the required inputs (for which the output is passed\n",
    "                as proxy)\n",
    "        \"\"\"\n",
    "        # e_l = np.zeros_like(output)\n",
    "        # if len(output.shape) == 1:\n",
    "        #     e_l[y_true] = 1.\n",
    "        #     return output[y_true] * (e_l - output)\n",
    "        # else:\n",
    "        #     for i,y in enumerate(y_true):\n",
    "        #         e_l[i][y] = 1.\n",
    "        #     relevant_outputs = np.array([output[i][y] for i,y in enumerate(y_true)]).reshape(-1, 1)\n",
    "        #     return relevant_outputs * (e_l - output)\n",
    "        if len(output.shape) == 1:\n",
    "            return output * np.eye(len(output)) - np.matmul(output.reshape(-1, 1), output.reshape(1, -1))\n",
    "        else:\n",
    "            return np.array([op * np.eye(len(op)) - np.matmul(op.reshape(-1, 1), op.reshape(1, -1)) for op in output])\n",
    "        \n",
    "# Attempt Xavier initialization and check if gradient problem goes away\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "    \"\"\"\n",
    "    Implements the cross entropy loss, L(y_hat, y) = -log(y_hat[y]) [where y is the true class label, from 0 to num_classes-1]\n",
    "\n",
    "    Arg: eps (float, optional): quantity added to predicted probability for stability during log and division.\n",
    "        Deafults to 1e-8.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float=1e-8) -> None:\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred: np.array, y_true) -> np.float64:\n",
    "        \"\"\"\n",
    "        Returns the loss for predicted probabilities y_pred, with true class label y_true\n",
    "\n",
    "        Args:\n",
    "            y_pred (np.array): The predicted probabilities of each of the classes. \n",
    "                Array of shape (num_classes,) or (num_samples, num_classes).\n",
    "            y_true: The true class label(s) of the data point(s). Could be a scalar or an array of shape (num_samples,).\n",
    "\n",
    "        Returns:\n",
    "            The (average) cross entropy loss (across all samples), a np.float64 object.\n",
    "        \"\"\"\n",
    "        if len(y_pred.shape) == 1:\n",
    "            # return -np.log(np.minimum(y_pred[y_true] + self.eps, 1.))\n",
    "            return -np.log(y_pred[y_true])\n",
    "        else:\n",
    "            return np.mean(np.array([\n",
    "                # -np.log(np.minimum(y_pred[i][y] + self.eps, 1.)) for i,y in enumerate(y_true)\n",
    "                -np.log(y_pred[i][y]) for i,y in enumerate(y_true)\n",
    "            ]))\n",
    "    \n",
    "    def backward(self, y_pred: np.array, y_true) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns the derivative of the loss wrt the predicted probability of the true class y_true\n",
    "\n",
    "        Args:\n",
    "            y_pred (np.array): The predicted probabilities of each of the classes. \n",
    "                Array of shape (num_classes,) or (num_samples, num_classes).\n",
    "            y_true: The true class label(s) of the data point(s). Could be a scalar or an array of shape (num_samples,).\n",
    "\n",
    "        Returns:\n",
    "            grad (np.array): The gradient of the loss wrt the inputs (probabilties) given to the loss function.\n",
    "                Array of shape matching y_pred.\n",
    "        \"\"\"\n",
    "        grad = np.zeros_like(y_pred)\n",
    "        if len(y_pred.shape) == 1:\n",
    "            # grad[y_true] = -1 / (np.minimum(y_pred[y_true] + self.eps, 1.))\n",
    "            grad[y_true] = -1 / y_pred[y_true]\n",
    "        else:\n",
    "            for i,y in enumerate(y_true):\n",
    "                # grad[i][y] = -1 / (np.minimum(y_pred[i][y] + self.eps, 1.))\n",
    "                grad[i][y] = -1 / y_pred[i][y]\n",
    "        return grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(Y_true, Y_pred) -> float:\n",
    "    if len(Y_pred.shape) == 1:\n",
    "        return 1. if np.argmax(Y_pred) == Y_true else 0.\n",
    "    else:\n",
    "        Y_pred_class = np.argmax(Y_pred, axis=1)\n",
    "        return np.sum(Y_pred_class == Y_true) / len(Y_true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    Models a single layer of a feedforward neural network. \n",
    "\n",
    "    Args:\n",
    "        input_size (int): number of inputs to the layer\n",
    "        output_size (int): number of outputs produced by the layer\n",
    "        activation (str, optional): activation function used by the layer. \n",
    "            Allowed activations: 'identity', 'sigmoid', 'tanh', 'relu', 'softmax'\n",
    "            Defaults to 'sigmoid'.\n",
    "        weight_init (str, optional): type of initialization to be performed for the weights and biases\n",
    "            Allowed initializations: 'random', 'xavier'\n",
    "            Defaults to 'random'.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        activation = 'sigmoid',\n",
    "        weight_init: str = 'random'\n",
    "    ) -> None:\n",
    "        if weight_init == 'xavier':\n",
    "            self.w = np.random.normal(scale=np.sqrt(2/(input_size + output_size)), size=(output_size, input_size))\n",
    "            self.b = np.random.normal(scale=np.sqrt(2/(input_size + output_size)), size=(output_size,))\n",
    "        else:\n",
    "            self.w = np.random.normal(size=(output_size, input_size))   # shape: (output_size, input_size)\n",
    "            self.b = np.random.normal(size=(output_size,))  # shape: (output_size,)\n",
    "        self.grad_w = np.zeros_like(self.w)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "        if activation == 'identity' or activation.__class__ == Identity:\n",
    "            self.activation = Identity()\n",
    "        elif activation == 'sigmoid' or activation.__class__ == Sigmoid:\n",
    "            self.activation = Sigmoid()\n",
    "        elif activation == 'tanh' or activation.__class__ == Tanh:\n",
    "            self.activation = Tanh()\n",
    "        elif activation == 'relu' or activation.__class__ == ReLU:\n",
    "            self.activation = ReLU()\n",
    "        elif activation == 'softmax' or activation.__class__ == Softmax:\n",
    "            self.activation = Softmax()\n",
    "    \n",
    "    def forward(self, x: np.array, eval_mode: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes the output of the layer for given input.\n",
    "\n",
    "        Args: \n",
    "            x (np.array): the input vector for the layer, of size (input_size,) or (num_samples, input_size).\n",
    "            eval_mode (bool, optional): whether to compute in eval mode, without storing input, pre-activation and output.\n",
    "                Defaults to False.\n",
    "        Returns: h (np.array): the output produced by the layer, of size (output_size,) or (num_samples, output_size).\n",
    "        \"\"\"\n",
    "        if not eval_mode:\n",
    "            self.input = x\n",
    "            self.pre_activation = np.matmul(x, self.w.T) + self.b\n",
    "            self.output = self.activation.forward(self.pre_activation)\n",
    "            return self.output\n",
    "        else:\n",
    "            return self.activation.forward(np.matmul(x, self.w.T) + self.b)\n",
    "    \n",
    "    def backward(self, accumulated_grads: np.array, y_true=None, w_next: np.array = None) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes the relevant necessary gradients of the layer, given the gradient accumulated until the succeeding layer.\n",
    "\n",
    "        Args:\n",
    "            accumulated_grads (np.array): gradient accumulated from the loss until the following layer, i.e. grad(L) wrt a_(i+1)\n",
    "                Array of shape (num_features,) or (num_samples, num_features).\n",
    "            y_true (optional): compatiblity feature, required only for the output layer for softmax activation.\n",
    "                Defaults to None.\n",
    "            w_next (np.array, optional): weight associated with the succeeding layer. Not applicable for the output layer.\n",
    "                Defaults to None.\n",
    "        \n",
    "        Returns: grad_to_return (np.array), gradient accumulated until the current layer, i.e., grad(L) wrt a_i\n",
    "        \"\"\"\n",
    "        # accumulated_grads ~ grad(L) wrt a_i+1\n",
    "        activation_grad = self.activation.backward(self.output, y_true)\n",
    "        if w_next is not None:\n",
    "            grad_to_return = np.matmul(accumulated_grads, w_next) * activation_grad\n",
    "        else:\n",
    "            if len(self.output.shape) == 1:\n",
    "                grad_to_return = np.matmul(accumulated_grads.reshape(1, -1), activation_grad)\n",
    "            else:\n",
    "                grad_to_return = np.array([np.matmul(accumulated_grads[i].reshape(1, -1), activation_grad[i]).reshape(-1) for i in range(len(accumulated_grads))])\n",
    "            # grad_to_return = accumulated_grads * activation_grad # shape = self.output.shape\n",
    "        # grad_to_return ~ grad(L) wrt a_i\n",
    "        self.grad_w += np.matmul(grad_to_return.T.reshape(self.w.shape[0], -1), self.input.reshape(-1, self.w.shape[1])) / self.input.reshape(-1, self.w.shape[1]).shape[0]\n",
    "        if len(self.input.shape) == 1:\n",
    "            self.grad_b += grad_to_return\n",
    "        else:\n",
    "            self.grad_b += np.mean(grad_to_return, axis=0)\n",
    "        return grad_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork():\n",
    "    \"\"\"\n",
    "    Models a vanilla feedforward neural network, with softmax activation for the outputs.\n",
    "\n",
    "    Args:\n",
    "        num_layers (int): number of hidden layers in the network\n",
    "        hidden_size (int): number of neurons per hidden layer\n",
    "        input_size (int): size/dimension of inputs given to the network. \n",
    "            Defaults to 784, flattened size of mnist/fashion-mnist data.\n",
    "        output_size (int): number of outputs produced by the network.\n",
    "            Defaults to 10, number of classes of mnist/fashion-mnist data.\n",
    "        activation (str, optional): activation function for the hidden layers.\n",
    "            Allowed activations: 'identity', 'sigmoid', 'tanh', 'relu'\n",
    "            Defaults to 'sigmoid'.\n",
    "        weight_init (str, optional): type of initialization to be performed for the weights and biases\n",
    "            Allowed initializations: 'random', 'xavier'\n",
    "            Defaults to 'random'.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        hidden_size: int,\n",
    "        input_size: int = 784,\n",
    "        output_size: int = 10,\n",
    "        activation = 'sigmoid',\n",
    "        weight_init: str = 'random'\n",
    "    ) -> None:\n",
    "        self.input_layer = Layer(input_size, hidden_size, activation, weight_init)\n",
    "        self.hidden_layers = []\n",
    "        for i in range(num_layers):\n",
    "            self.hidden_layers.append(Layer(hidden_size, hidden_size, activation, weight_init))\n",
    "        self.output_layer = Layer(hidden_size, output_size, 'softmax', weight_init)\n",
    "    \n",
    "    def forward(self, x: np.array, eval_mode: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes the output of the network for given input.\n",
    "\n",
    "        Args: \n",
    "            x (numpy.array): the input to the network, of size (input_size,) or (num_samples, input_size)\n",
    "            eval_mode (bool, optional): whether to compute in eval mode, without storing input, pre-activation and output\n",
    "            in each layer. Defaults to False.\n",
    "        Returns: y_hat (numpy.array): the output produced by the network, of size (output_size,) or (num_samples, output_size)\n",
    "        \"\"\"\n",
    "        result = self.input_layer.forward(x, eval_mode)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            result = hidden_layer.forward(result, eval_mode)\n",
    "        return self.output_layer.forward(result, eval_mode)\n",
    "    \n",
    "    def backward(self, accumulated_grads: np.array, y_true) -> None:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss wrt all the parameters (weights, biases) of the network, given the gradient of the loss\n",
    "        wrt the output of the network.\n",
    "\n",
    "        Args:\n",
    "            accumulated_grads (np.array): gradient of the loss wrt the outputs of the network\n",
    "                Array of size (num_classes,) or (num_samples, num_classes).\n",
    "            y_true: the true class label(s) for the data input(s) to the network\n",
    "        \"\"\"\n",
    "        self.y_true = y_true\n",
    "        accumulated_grads = self.output_layer.backward(accumulated_grads, y_true)\n",
    "        w_next = self.output_layer.w\n",
    "        for hidden_layer in reversed(self.hidden_layers):\n",
    "            accumulated_grads = hidden_layer.backward(accumulated_grads, w_next=w_next)\n",
    "            w_next = hidden_layer.w\n",
    "        self.input_layer.backward(accumulated_grads, w_next=w_next)\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"\n",
    "        Sets the gradients of all the parameters of the network to zero.\n",
    "        \"\"\"\n",
    "        self.output_layer.grad_w, self.output_layer.grad_b = 0., 0.\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden_layer.grad_w, hidden_layer.grad_b = 0., 0.\n",
    "        self.input_layer.grad_w, self.input_layer.grad_b = 0., 0.\n",
    "    \n",
    "    def update_weights(self, lr: float = 1e-3) -> None: # DEPRECATE\n",
    "        \"\"\"\n",
    "        Updates the parameters of the network along the direction of the negative gradient.\n",
    "\n",
    "        Arg: lr (float, optional): learning rate for the update. Defaults to 1e-3.\n",
    "        \"\"\"\n",
    "        self.output_layer.w -= lr * self.output_layer.grad_w\n",
    "        self.output_layer.b -= lr * self.output_layer.grad_b\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden_layer.w -= lr * hidden_layer.grad_w\n",
    "            hidden_layer.b -= lr * hidden_layer.grad_b\n",
    "        self.input_layer.w -= lr * self.input_layer.grad_w\n",
    "        self.input_layer.b -= lr * self.input_layer.grad_b\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent():\n",
    "    \"\"\"\n",
    "    Class to implement stochastic (mini-batch vanilla) gradient descent optimization algorithm on a model.\n",
    "\n",
    "    Args:\n",
    "        model (FeedForwardNeuralNetwork): The model whose parameters are to be learned.\n",
    "        lr (float, optional): The learning rate for the updates. Defaults to 1e-3.\n",
    "        weight_decay (float, optional): The L2-regularization to use for the parameters of the network.\n",
    "            Defaults to 0.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: FeedForwardNeuralNetwork,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Performs one step/update to the parameters of the model, using grads computed by the model's backward function.\n",
    "        \"\"\"\n",
    "        self.model.output_layer.w -= self.lr * (self.model.output_layer.grad_w + self.weight_decay * self.model.output_layer.w)\n",
    "        self.model.output_layer.b -= self.lr * (self.model.output_layer.grad_b + self.weight_decay * self.model.output_layer.b)\n",
    "        for hidden_layer in self.model.hidden_layers:\n",
    "            hidden_layer.w -= self.lr * (hidden_layer.grad_w + self.weight_decay * hidden_layer.w)\n",
    "            hidden_layer.b -= self.lr * (hidden_layer.grad_b + self.weight_decay * hidden_layer.b)\n",
    "        self.model.input_layer.w -= self.lr * (self.model.input_layer.grad_w + self.weight_decay * self.model.input_layer.w)\n",
    "        self.model.input_layer.b -= self.lr * (self.model.input_layer.grad_b + self.weight_decay * self.model.input_layer.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumGradientDescent():\n",
    "    \"\"\"\n",
    "    Class to implement momentum-based gradient descent optimization algorithm on a model.\n",
    "\n",
    "    Args:\n",
    "        model (FeedForwardNeuralNetwork): The model whose parameters are to be learned.\n",
    "        lr (float, optional): The learning rate for the updates. Defaults to 1e-3.\n",
    "        beta (float, optional): The momentum to be used, quantifying the confidence in the history of updates.\n",
    "            Should range from 0 to 1. Defaults to 0.9.\n",
    "        weight_decay (float, optional): The L2-regularization to use for the parameters of the network.\n",
    "            Defaults to 0.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: FeedForwardNeuralNetwork,\n",
    "        lr: float = 1e-3,\n",
    "        beta: float = 0.9,\n",
    "        weight_decay: float = 0.\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.beta = beta\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.input_u_w = np.zeros_like(model.input_layer.w)\n",
    "        self.input_u_b = np.zeros_like(model.input_layer.b)\n",
    "        self.hidden_u_ws = []\n",
    "        self.hidden_u_bs = []\n",
    "        for hidden_layer in model.hidden_layers:\n",
    "            self.hidden_u_ws.append(np.zeros_like(hidden_layer.w))\n",
    "            self.hidden_u_bs.append(np.zeros_like(hidden_layer.b))\n",
    "        self.output_u_w = np.zeros_like(model.output_layer.w)\n",
    "        self.output_u_b = np.zeros_like(model.output_layer.b)\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Performs one step/update to the parameters of the model, using grads computed by the model's backward function.\n",
    "        \"\"\"\n",
    "        self.output_u_w = self.beta * self.output_u_w + self.lr * (self.model.output_layer.grad_w + self.weight_decay * self.model.output_layer.w)\n",
    "        self.model.output_layer.w -= self.output_u_w\n",
    "        self.output_u_b = self.beta * self.output_u_b + self.lr * (self.model.output_layer.grad_b + self.weight_decay * self.model.output_layer.b)\n",
    "        self.model.output_layer.b -= self.output_u_b\n",
    "        for hidden_layer, hidden_u_w, hidden_u_b in zip(self.model.hidden_layers, self.hidden_u_ws, self.hidden_u_bs):\n",
    "            hidden_u_w = self.beta * hidden_u_w + self.lr * (hidden_layer.grad_w + self.weight_decay * hidden_layer.w)\n",
    "            hidden_layer.w -= hidden_u_w\n",
    "            hidden_u_b = self.beta * hidden_u_b + self.lr * (hidden_layer.grad_b + self.weight_decay * hidden_layer.b)\n",
    "            hidden_layer.b -= hidden_u_b\n",
    "        self.input_u_w = self.beta * self.input_u_w + self.lr * (self.model.input_layer.grad_w + self.weight_decay * self.model.input_layer.w)\n",
    "        self.model.input_layer.w -= self.input_u_w\n",
    "        self.input_u_b = self.beta * self.input_u_b + self.lr * (self.model.input_layer.grad_b + self.weight_decay * self.model.input_layer.b)\n",
    "        self.model.input_layer.b -= self.input_u_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovGradientDescent():\n",
    "    \"\"\"\n",
    "    Class to implement Nesterov accelerated gradient descent optimization algorithm on a model.\n",
    "\n",
    "    Args:\n",
    "        model (FeedForwardNeuralNetwork): The model whose parameters are to be learned.\n",
    "        lr (float, optional): The learning rate for the updates. Defaults to 1e-3.\n",
    "        beta (float, optional): The momentum to be used, quantifying the confidence in the history of updates (and the lookahead).\n",
    "            Should range from 0 to 1. Defaults to 0.9.\n",
    "        loss_fn (Any, optional): The loss function used in training the model, used to calculate the lookahead gradient.\n",
    "            Defaults to CrossEntropyLoss().\n",
    "        weight_decay (float, optional): The L2-regularization to use for the parameters of the network.\n",
    "            Defaults to 0.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: FeedForwardNeuralNetwork,\n",
    "        lr: float = 1e-3,\n",
    "        beta: float = 0.9,\n",
    "        loss_fn = CrossEntropyLoss(),\n",
    "        weight_decay : float = 0.\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.loss_fn = loss_fn\n",
    "        self.weight_decay = weight_decay\n",
    "        self.input_u_w = np.zeros_like(model.input_layer.w)\n",
    "        self.input_u_b = np.zeros_like(model.input_layer.b)\n",
    "        self.hidden_u_ws = []\n",
    "        self.hidden_u_bs = []\n",
    "        for hidden_layer in model.hidden_layers:\n",
    "            self.hidden_u_ws.append(np.zeros_like(hidden_layer.w))\n",
    "            self.hidden_u_bs.append(np.zeros_like(hidden_layer.b))\n",
    "        self.output_u_w = np.zeros_like(model.output_layer.w)\n",
    "        self.output_u_b = np.zeros_like(model.output_layer.b)\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Performs one step/update to the parameters of the model, using grads computed by the model's backward function.\n",
    "        \"\"\"\n",
    "        projected_model = FeedForwardNeuralNetwork(\n",
    "            num_layers=len(self.model.hidden_layers),\n",
    "            hidden_size=self.model.input_layer.b.shape[0],\n",
    "            input_size=self.model.input_layer.w.shape[1],\n",
    "            output_size=self.model.output_layer.b.shape[0],\n",
    "            activation=self.model.input_layer.activation\n",
    "        )\n",
    "        projected_model.input_layer.w = self.model.input_layer.w - self.beta * self.input_u_w\n",
    "        projected_model.input_layer.b = self.model.input_layer.b - self.beta * self.input_u_b\n",
    "        for projected_hidden_layer, curr_hidden_layer, hidden_u_w, hidden_u_b in zip(projected_model.hidden_layers, self.model.hidden_layers, self.hidden_u_ws, self.hidden_u_bs):\n",
    "            projected_hidden_layer.w = curr_hidden_layer.w - self.beta * hidden_u_w\n",
    "            projected_hidden_layer.b = curr_hidden_layer.b - self.beta * hidden_u_b\n",
    "        projected_model.output_layer.w = self.model.output_layer.w - self.beta * self.output_u_w\n",
    "        projected_model.output_layer.b = self.model.output_layer.b - self.beta * self.output_u_b\n",
    "        # Model with lookahead parameters\n",
    "\n",
    "        y_pred = projected_model.forward(self.model.input_layer.input)\n",
    "        accumulated_grads = self.loss_fn.backward(y_pred, self.model.y_true)\n",
    "        projected_model.backward(accumulated_grads, self.model.y_true)\n",
    "        # Lookahead gradient computation\n",
    "\n",
    "        self.input_u_w = self.beta * self.input_u_w + self.lr * (projected_model.input_layer.grad_w + self.weight_decay * self.model.input_layer.w)\n",
    "        self.model.input_layer.w -= self.input_u_w\n",
    "        self.input_u_b = self.beta * self.input_u_b + self.lr * (projected_model.input_layer.grad_b + self.weight_decay * self.model.input_layer.b)\n",
    "        self.model.input_layer.b -= self.input_u_b\n",
    "        for hidden_layer, projected_hidden_layer, hidden_u_w, hidden_u_b in zip(self.model.hidden_layers, projected_model.hidden_layers, self.hidden_u_ws, self.hidden_u_bs):\n",
    "            hidden_u_w = self.beta * hidden_u_w + self.lr * (projected_hidden_layer.grad_w + self.weight_decay * hidden_layer.w)\n",
    "            hidden_layer.w -= hidden_u_w\n",
    "            hidden_u_b = self.beta * hidden_u_b + self.lr * (projected_hidden_layer.grad_b + self.weight_decay * hidden_layer.b)\n",
    "            hidden_layer.b -= hidden_u_b\n",
    "        self.output_u_w = self.beta * self.output_u_w + self.lr * (projected_model.output_layer.grad_w + self.weight_decay ( self.model.output_layer.w))\n",
    "        self.model.output_layer.w -= self.output_u_w\n",
    "        self.output_u_b = self.beta * self.output_u_b + self.lr * (projected_model.output_layer.grad_b + self.weight_decay ( self.model.output_layer.b))\n",
    "        self.model.output_layer.b -= self.output_u_b\n",
    "        # Parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp():\n",
    "    \"\"\"\n",
    "    Class to implement RMSProp optimization algorithm on a model.\n",
    "\n",
    "    Args:\n",
    "        model (FeedForwardNeuralNetwork): The model whose parameters are to be learned.\n",
    "        lr (float, optional): The learning rate for the updates. Defaults to 1e-3.\n",
    "        beta (float, optional): The momentum to be used, quantifying the confidence in the history of updates.\n",
    "            Should range from 0 to 1. Defaults to 0.9.\n",
    "        eps(float, optional): The amount to be added to the accumulated history, to add stability to computation.\n",
    "            Defaults to 1e-8.\n",
    "        weight_decay (float, optional): The L2-regularization to use for the parameters of the network.\n",
    "            Defaults to 0.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: FeedForwardNeuralNetwork,\n",
    "            lr: float = 1e-3,\n",
    "            beta: float = 0.9,\n",
    "            eps: float = 1e-8,\n",
    "            weight_decay: float = 0.\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.input_v_w = np.zeros_like(model.input_layer.w)\n",
    "        self.input_v_b = np.zeros_like(model.input_layer.b)\n",
    "        self.hidden_v_ws = []\n",
    "        self.hidden_v_bs = []\n",
    "        for hidden_layer in model.hidden_layers:\n",
    "            self.hidden_v_ws.append(np.zeros_like(hidden_layer.w))\n",
    "            self.hidden_v_bs.append(np.zeros_like(hidden_layer.b))\n",
    "        self.output_v_w = np.zeros_like(model.output_layer.w)\n",
    "        self.output_v_b = np.zeros_like(model.output_layer.b)\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Performs one step/update to the parameters of the model, using grads computed by the model's backward function.\n",
    "        \"\"\"\n",
    "        self.input_v_w = self.beta * self.input_v_w + (1 - self.beta) * self.model.input_layer.grad_w**2\n",
    "        self.model.input_layer.w -= self.lr * (self.model.input_layer.grad_w + self.weight_decay * self.model.input_layer.w) / np.sqrt(self.input_v_w + self.eps)\n",
    "        self.input_v_b = self.beta * self.input_v_b + (1 - self.beta) * self.model.input_layer.grad_b**2 \n",
    "        self.model.input_layer.b -= self.lr * (self.model.input_layer.grad_b + self.weight_decay * self.model.input_layer.b) / np.sqrt(self.input_v_b + self.eps)\n",
    "        for hidden_layer, hidden_v_w, hidden_v_b in zip(self.model.hidden_layers, self.hidden_v_ws, self.hidden_v_bs):\n",
    "            hidden_v_w = self.beta * hidden_v_w + (1 - self.beta) * hidden_layer.grad_w**2\n",
    "            hidden_layer.w -= self.lr * (hidden_layer.grad_w + self.weight_decay * hidden_layer.w) / np.sqrt(hidden_v_w + self.eps)\n",
    "            hidden_v_b = self.beta * hidden_v_b + (1 - self.beta) * hidden_layer.grad_b**2\n",
    "            hidden_layer.b -= self.lr * (hidden_layer.grad_b + self.weight_decay * hidden_layer.b) / np.sqrt(hidden_v_b + self.eps)\n",
    "        self.output_v_w = self.beta * self.output_v_w + (1 - self.beta) * self.model.output_layer.grad_w**2\n",
    "        self.model.output_layer.w -= self.lr * (self.model.output_layer.grad_w + self.weight_decay * self.model.output_layer.w) / np.sqrt(self.output_v_w + self.eps)\n",
    "        self.output_v_b = self.beta * self.output_v_b + (1 - self.beta) * self.model.output_layer.grad_b**2 \n",
    "        self.model.output_layer.b -= self.lr * (self.model.output_layer.grad_b + self.weight_decay * self.model.output_layer.b) / np.sqrt(self.output_v_b + self.eps)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def train(\n",
    "    model: FeedForwardNeuralNetwork,\n",
    "    X_train: np.array,\n",
    "    Y_train: np.array,\n",
    "    X_val: np.array,\n",
    "    Y_val: np.array,\n",
    "    optimizer,\n",
    "    loss_fn = CrossEntropyLoss(),\n",
    "    metric = categorical_accuracy,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64\n",
    ") -> Dict[str, List]:\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'train_score': [],\n",
    "        'val_loss': [],\n",
    "        'val_score': []\n",
    "    }\n",
    "    NUM_BATCHES = int(np.ceil(len(X_train)/batch_size))\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_score = 0.0\n",
    "        for batch in range(NUM_BATCHES):\n",
    "            X_train_batch = X_train[batch*batch_size:np.minimum(batch_size*(batch+1), len(X_train))]\n",
    "            Y_train_batch = Y_train[batch*batch_size:np.minimum(batch_size*(batch+1), len(X_train))]\n",
    "            model.zero_grad()\n",
    "            Y_pred_train = model.forward(X_train_batch)\n",
    "            loss = loss_fn.forward(Y_pred_train, Y_train_batch)\n",
    "            loss_grad = loss_fn.backward(Y_pred_train, Y_train_batch)\n",
    "            model.backward(loss_grad, Y_train_batch)\n",
    "            optimizer.step()\n",
    "            train_loss += loss / NUM_BATCHES\n",
    "            train_score += metric(Y_train_batch, Y_pred_train) / NUM_BATCHES\n",
    "        Y_pred_val = model.forward(X_val, True)\n",
    "        val_loss = loss_fn.forward(Y_pred_val, Y_val)\n",
    "        val_score = metric(Y_val, Y_pred_val)\n",
    "        history['epoch'].append(epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_score'].append(train_score)\n",
    "        history['val_score'].append(val_score)\n",
    "        print(f'[Epoch {epoch+1}/{epochs}] train loss: {train_loss:.6f}, val loss: {val_loss: .6f} || train score: {train_score:.6f}, val score: {val_score:.6f}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "        model: FeedForwardNeuralNetwork,\n",
    "        X: np.array\n",
    "):\n",
    "        return model.forward(X, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] train loss: 1.136888, val loss:  0.765665 || train score: 0.616710, val score: 0.720000\n",
      "[Epoch 2/10] train loss: 0.750825, val loss:  0.674554 || train score: 0.727007, val score: 0.753000\n",
      "[Epoch 3/10] train loss: 0.677502, val loss:  0.630360 || train score: 0.752293, val score: 0.763000\n",
      "[Epoch 4/10] train loss: 0.634801, val loss:  0.595877 || train score: 0.767663, val score: 0.781000\n",
      "[Epoch 5/10] train loss: 0.605114, val loss:  0.569434 || train score: 0.779168, val score: 0.787000\n",
      "[Epoch 6/10] train loss: 0.582738, val loss:  0.550163 || train score: 0.787180, val score: 0.797000\n",
      "[Epoch 7/10] train loss: 0.564792, val loss:  0.534685 || train score: 0.793582, val score: 0.799000\n",
      "[Epoch 8/10] train loss: 0.549729, val loss:  0.521078 || train score: 0.798946, val score: 0.801000\n",
      "[Epoch 9/10] train loss: 0.536732, val loss:  0.508815 || train score: 0.803621, val score: 0.805000\n",
      "[Epoch 10/10] train loss: 0.525303, val loss:  0.497730 || train score: 0.807786, val score: 0.810000\n"
     ]
    }
   ],
   "source": [
    "model = FeedForwardNeuralNetwork(4, 128, activation='sigmoid')\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = StochasticGradientDescent(model, 0.1)\n",
    "metric = categorical_accuracy\n",
    "\n",
    "history = train(\n",
    "    model,\n",
    "    X_train.reshape(-1, 784),\n",
    "    Y_train,\n",
    "    X_train.reshape(-1, 784)[59000:],\n",
    "    Y_train[59000:],\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    metric,\n",
    "    epochs=10,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small-scale test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 2), (200,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "X_train_small_unscaled = rng.integers(-10, 30, (200, 2))\n",
    "Y_train_small = np.matmul(X_train_small_unscaled, np.array([1, -4])) % 2\n",
    "X_train_small_unscaled.shape, Y_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small_scaled = (X_train_small_unscaled + 10) / 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(384)\n",
    "\n",
    "model = FeedForwardNeuralNetwork(num_layers=1, hidden_size=2, input_size=2, output_size=2, activation='sigmoid', weight_init='xavier')\n",
    "count = 0\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.49273956  0.65380943]\n",
      " [ 0.65365887  0.54351532]] [-1.27213124 -0.31322795]\n",
      "[[ 0.70947346 -0.10920137]\n",
      " [-0.80874811 -0.46250622]] [0.28798326 1.30528329]\n",
      "[[ 0.54584889 -1.05623667]\n",
      " [-1.02850766  0.67841867]] [ 0.88357116 -0.19373288]\n"
     ]
    }
   ],
   "source": [
    "print(model.input_layer.w, model.input_layer.b)\n",
    "print(model.hidden_layers[0].w, model.hidden_layers[0].b)\n",
    "print(model.output_layer.w, model.output_layer.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.225, 0.75 ]), 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_small_scaled[0], Y_train_small[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00940206 0.0313402 ]\n",
      " [0.00154585 0.00515285]] [0.04178694 0.00687046]\n",
      "[[ 0.04968094  0.09576247]\n",
      " [-0.0292482  -0.05637734]] [ 0.17098035 -0.10065966]\n",
      "[[ 0.          0.        ]\n",
      " [-0.42255692 -0.48232191]] [ 0.         -0.69661882]\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "Y_pred = model.forward(X_train_small_scaled[0])\n",
    "loss = loss_fn.forward(Y_pred, Y_train_small[0])\n",
    "loss_grad = loss_fn.backward(Y_pred, Y_train_small[0])\n",
    "model.backward(loss_grad, Y_train_small[0])\n",
    "print(model.input_layer.grad_w, model.input_layer.grad_b)\n",
    "print(model.hidden_layers[0].grad_w, model.hidden_layers[0].grad_b)\n",
    "print(model.output_layer.grad_w, model.output_layer.grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69661882 0.30338118]\n"
     ]
    }
   ],
   "source": [
    "print(model.output_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69661882, -0.69661882])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_aout = - (np.array([0, 1]) - model.output_layer.output)\n",
    "grad_aout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42255692,  0.48232191],\n",
       "       [-0.42255692, -0.48232191]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_wout = np.matmul(grad_aout.reshape(-1, 1), model.output_layer.input.reshape(-1, 1).T)\n",
    "grad_wout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = StochasticGradientDescent(model, lr=0.1)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_details(model: FeedForwardNeuralNetwork):\n",
    "    print('Input layer weight:', model.input_layer.w)\n",
    "    print('Input layer bias:', model.input_layer.b)\n",
    "    print('Hidden layer weight:', model.hidden_layers[0].w)\n",
    "    print('Hidden layer bias:', model.hidden_layers[0].b)\n",
    "    print('Output layer weight:', model.output_layer.w)\n",
    "    print('Output layer bias:', model.output_layer.b)\n",
    "    \n",
    "    print('Input layer weight gradient:', model.input_layer.grad_w)\n",
    "    print('Input layer bias gradient:', model.input_layer.grad_b)\n",
    "    print('Hidden layer weight gradient:', model.hidden_layers[0].grad_w)\n",
    "    print('Hidden layer bias gradient:', model.hidden_layers[0].grad_b)\n",
    "    print('Output layer weight gradient:', model.output_layer.grad_w)\n",
    "    print('Output layer bias gradient:', model.output_layer.grad_b)\n",
    "\n",
    "def print_intermediates(model: FeedForwardNeuralNetwork):\n",
    "    print('Input:', model.input_layer.input)\n",
    "    print('Input layer pre-activation:', model.input_layer.pre_activation)\n",
    "    print('Input layer output:', model.input_layer.output)\n",
    "    print('Hidden layer pre-activation:', model.hidden_layers[0].pre_activation)\n",
    "    print('Hidden layer output:', model.hidden_layers[0].output)\n",
    "    print('Output layer pre-activation:', model.output_layer.pre_activation)\n",
    "    print('Output layer output:', model.output_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer weight: [[-0.27332273  0.36266822  0.3625847 ]\n",
      " [ 0.30148805 -0.70565145 -0.17374761]\n",
      " [ 0.39354507 -0.06057402 -0.44861274]\n",
      " [-0.25655229  0.15974437  0.7240409 ]\n",
      " [ 0.30278249 -0.58589469 -0.5705134 ]\n",
      " [ 0.37631897  0.49011709 -0.10746367]\n",
      " [ 0.16734036  0.36289134  0.66128351]\n",
      " [ 0.00292486  0.34263207  0.44106116]\n",
      " [-0.01726162 -0.42624764 -0.15930964]\n",
      " [ 0.17258067 -0.86378157  0.37449577]]\n",
      "Input layer bias: [ 0.24171995  0.51779148 -0.88279604 -0.38321432  0.04128801 -0.86583483\n",
      "  0.30760588 -0.23250912 -0.27901413 -0.07009817]\n",
      "Hidden layer weight: [[-0.05196967 -0.21304195  0.02546332 -0.05107187 -0.0974044  -0.3417984\n",
      "  -0.29935158 -0.01507817 -0.12906296  0.03734594]\n",
      " [ 0.01016864  0.20483022  0.0137705  -0.08288059 -0.07911533  0.27757576\n",
      "   0.00289474 -0.44346363  0.44272316  0.50618818]\n",
      " [-0.17050262 -0.19000085  0.25875323  0.09947229  0.15963418 -0.25965466\n",
      "  -0.08067103 -0.33824162  0.20821238  0.13379946]\n",
      " [-0.12588571  0.08604619 -0.12006281 -0.10902088 -0.22340676 -0.03718557\n",
      "  -0.03449622  0.18463179 -0.12346026  0.22903191]\n",
      " [-0.01071008  0.26344687 -0.18812241  0.28650666  0.6044479   0.49361815\n",
      "   0.10691912  0.64545123 -0.0915794   0.46505914]\n",
      " [ 0.1920277  -0.28890364 -0.08824041  0.07774193 -0.07248327 -0.36348909\n",
      "   0.02451449 -0.10390475  0.36124015 -0.20521178]\n",
      " [ 0.01301512 -0.24839252  0.14855033  0.4072286  -0.03514063  0.30727573\n",
      "  -0.0467154  -0.65897054  0.04503982 -0.03156905]\n",
      " [-0.76184014 -0.1428349  -0.1611867   0.06773731 -0.03956908 -0.37460163\n",
      "  -0.04490739  0.13741148  0.38104751  0.03394468]\n",
      " [ 0.48197322  0.43565227  0.13644638 -0.27888148 -0.16671709 -0.03560132\n",
      "  -0.11403075 -0.69066185  0.03257425 -0.19702761]\n",
      " [ 0.00880766  0.07493871  0.40247017 -0.10682675 -0.09166834 -0.17606938\n",
      "  -0.11920926 -0.15002418  0.26860211  0.07930197]]\n",
      "Hidden layer bias: [-0.26241337 -0.04830386 -0.40065783  0.48477667 -0.0245746  -0.23366019\n",
      " -0.18383009 -0.2516496   0.20868787 -0.38472064]\n",
      "Output layer weight: [[-0.39614294  0.06256553 -0.26815826  0.42710121 -0.47330512  0.12400376\n",
      "  -0.16879813  0.06975403 -0.62595977 -0.13833946]\n",
      " [ 0.61796992 -0.107759   -0.40734236  0.45029053 -0.05986392 -0.33077998\n",
      "  -0.05017964 -0.5777634  -0.33098046 -0.14999075]\n",
      " [ 0.62513674  0.12395012 -0.59042693  0.21770982  0.26206606 -0.40287872\n",
      "   0.26667601  0.18958252  0.1138327  -0.18313697]\n",
      " [-0.22067413 -0.10154724 -0.07230313 -0.14035043  0.06564824 -0.04082299\n",
      "   0.85863606 -0.19715293  0.02304812  0.40452649]]\n",
      "Output layer bias: [-0.61915408 -0.01280169 -0.15228137 -0.03724808]\n",
      "Input layer weight gradient: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Input layer bias gradient: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Hidden layer weight gradient: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Hidden layer bias gradient: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Output layer weight gradient: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Output layer bias gradient: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print_model_details(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\t\t9.106202677964367\n",
      "Input layer weight: [[ 0.23479471  0.83785421  0.82983836]\n",
      " [ 1.21087674  0.12505234  0.64824358]\n",
      " [ 0.39354507 -0.06057402 -0.44861274]\n",
      " [-0.25650707  0.15064236  0.71526672]\n",
      " [ 0.27927454 -0.59238619 -0.57413352]\n",
      " [ 0.37631897  0.49011709 -0.10746367]\n",
      " [ 2.9899046   2.96094388  3.20328574]\n",
      " [ 1.71706631  1.91592647  1.98468433]\n",
      " [-0.01726162 -0.42624764 -0.15930964]\n",
      " [ 0.5203254  -0.63705244  0.70443233]]\n",
      "Input layer bias: [ 1.2363733   2.28490683 -0.88279604 -0.39484775  0.01212238 -0.86583483\n",
      "  5.82910282  3.11915881 -0.27901413  0.56558932]\n",
      "Hidden layer weight: [[-5.19696652e-02 -2.13041947e-01  2.54633176e-02 -5.10718744e-02\n",
      "  -9.74044048e-02 -3.41798403e-01 -2.99351575e-01 -1.50781660e-02\n",
      "  -1.29062957e-01  3.73459430e-02]\n",
      " [ 8.80163835e-03  2.01355054e-01  1.37704965e-02 -8.31211696e-02\n",
      "  -7.92498465e-02  2.77575759e-01 -2.09077755e-03 -4.44198613e-01\n",
      "   4.42723164e-01  5.05173337e-01]\n",
      " [-1.70502622e-01 -1.90000849e-01  2.58753232e-01  9.94722864e-02\n",
      "   1.59634178e-01 -2.59654656e-01 -8.06710310e-02 -3.38241618e-01\n",
      "   2.08212379e-01  1.33799458e-01]\n",
      " [ 1.61383387e+00  2.43871484e+00 -1.20062808e-01 -5.15383867e-03\n",
      "  -2.12561433e-01 -3.71855727e-02  7.73244062e+00  4.30597034e+00\n",
      "  -1.23460257e-01  6.98397412e-01]\n",
      " [ 2.30195346e-01  6.23738551e-01 -1.88122414e-01  2.94255305e-01\n",
      "   6.04996240e-01  4.93618153e-01  1.28530030e+00  1.32037975e+00\n",
      "  -9.15793965e-02  5.50980414e-01]\n",
      " [ 1.92027700e-01 -2.88903641e-01 -8.82404137e-02  7.77419307e-02\n",
      "  -7.24832738e-02 -3.63489094e-01  2.45144858e-02 -1.03904747e-01\n",
      "   3.61240152e-01 -2.05211781e-01]\n",
      " [ 1.30151189e-02 -2.48392516e-01  1.48550335e-01  4.07228600e-01\n",
      "  -3.51406268e-02  3.07275732e-01 -4.67154007e-02 -6.58970544e-01\n",
      "   4.50398207e-02 -3.15690533e-02]\n",
      " [-7.61840140e-01 -1.42834901e-01 -1.61186701e-01  6.77373140e-02\n",
      "  -3.95690770e-02 -3.74601631e-01 -4.49073890e-02  1.37411479e-01\n",
      "   3.81047506e-01  3.39446806e-02]\n",
      " [ 4.36574696e-01  3.86038674e-01  1.36446377e-01 -2.81183564e-01\n",
      "  -1.68385699e-01 -3.56013194e-02 -2.33389506e-01 -7.09177781e-01\n",
      "   3.25742463e-02 -2.00561396e-01]\n",
      " [ 8.80765512e-03  7.49387063e-02  4.02470168e-01 -1.06826750e-01\n",
      "  -9.16683394e-02 -1.76069379e-01 -1.19209260e-01 -1.50024183e-01\n",
      "   2.68602105e-01  7.93019730e-02]]\n",
      "Hidden layer bias: [-0.26241337 -0.05478503 -0.40065783  3.07858876  0.20122967 -0.23366019\n",
      " -0.18383009 -0.2516496   0.05405668 -0.38472064]\n",
      "Output layer weight: [[-0.39614294  0.06649336 -0.26815826  4.05665466  0.27106125  0.12400376\n",
      "  -0.16879813  0.06975403 -0.58824215 -0.13833946]\n",
      " [ 0.61796992 -0.10023211 -0.40734236  5.48526133  0.95974107 -0.33077998\n",
      "  -0.05017964 -0.5777634  -0.30380921 -0.14999075]\n",
      " [ 0.62513674  0.12838396 -0.59042693  5.09477895  1.25686544 -0.40287872\n",
      "   0.26667601  0.18958252  0.13462206 -0.18313697]\n",
      " [-0.22067413 -0.09575323 -0.07230313  5.0963297   1.12843639 -0.04082299\n",
      "   0.85863606 -0.19715293  0.05341531  0.40452649]]\n",
      "Output layer bias: [0.20952479 0.64770683 0.50395181 0.80416912]\n",
      "Input layer weight gradient: [[-1.62682101e+00 -1.48049659e+00 -1.47073968e+00]\n",
      " [-2.53158867e+00 -2.30520388e+00 -2.29007800e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.80626504e-02 -1.97612617e-02 -2.99935591e-02]\n",
      " [ 9.69335633e-03  2.13352876e-03  1.53620643e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-7.84939000e+00 -7.14425307e+00 -7.09721422e+00]\n",
      " [-4.52051853e+00 -4.11728771e+00 -4.09032230e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-8.60885377e-01 -7.38335194e-01 -7.80683102e-01]]\n",
      "Input layer bias gradient: [-3.16594849e+00 -4.92804126e+00  0.00000000e+00 -3.64690540e-02\n",
      "  1.20596261e-02  0.00000000e+00 -1.52765429e+01 -8.80075160e+00\n",
      "  0.00000000e+00 -1.63471597e+00]\n",
      "Hidden layer weight gradient: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-4.09077939e+00 -6.22607030e+00  0.00000000e+00 -1.00667521e-01\n",
      "  -8.59197492e-03  0.00000000e+00 -1.95850372e+01 -1.10807287e+01\n",
      "   0.00000000e+00 -1.50492705e+00]\n",
      " [-9.02823182e-01 -1.36718406e+00  0.00000000e+00 -2.27237198e-02\n",
      "  -1.82144482e-03  0.00000000e+00 -4.31272295e+00 -2.44105630e+00\n",
      "   0.00000000e+00 -3.29599943e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "Hidden layer bias gradient: [ 0.          0.          0.         -2.56245534 -0.56331575  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Output layer weight gradient: [[  0.           0.           0.          -0.09330793  -0.01796599\n",
      "    0.           0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.         -15.99956525  -3.09805576\n",
      "    0.           0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.         -14.04724572  -2.71787686\n",
      "    0.           0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.         -17.41898825  -3.36969106\n",
      "    0.           0.           0.           0.           0.        ]]\n",
      "Output layer bias gradient: [-0.0015204  -0.23876914 -0.20499939 -0.26      ]\n",
      "Input: [[0.325 0.475 0.175]\n",
      " [0.8   0.475 0.05 ]\n",
      " [0.475 0.05  0.45 ]\n",
      " [0.775 0.325 0.625]\n",
      " [0.475 0.25  0.875]\n",
      " [0.6   0.725 0.75 ]\n",
      " [0.4   0.825 0.075]\n",
      " [0.275 0.35  0.   ]\n",
      " [0.225 0.25  0.2  ]\n",
      " [0.7   0.6   0.375]\n",
      " [0.85  0.025 0.475]\n",
      " [0.425 0.275 0.075]\n",
      " [0.875 0.625 0.425]\n",
      " [0.8   0.375 0.2  ]\n",
      " [0.225 0.775 0.425]\n",
      " [0.425 0.275 0.55 ]\n",
      " [0.1   0.825 0.775]\n",
      " [0.825 0.25  0.075]\n",
      " [0.375 0.4   0.375]\n",
      " [0.1   0.025 0.55 ]\n",
      " [0.675 0.325 0.45 ]\n",
      " [0.875 0.925 0.65 ]\n",
      " [0.55  0.3   0.95 ]\n",
      " [0.825 0.825 0.85 ]\n",
      " [0.8   0.75  0.1  ]\n",
      " [0.    0.975 0.125]\n",
      " [0.8   0.775 0.275]\n",
      " [0.325 0.75  0.225]\n",
      " [0.125 0.6   0.325]\n",
      " [0.875 0.425 0.55 ]\n",
      " [0.05  0.375 0.15 ]\n",
      " [0.925 0.075 0.3  ]\n",
      " [0.15  0.65  0.425]\n",
      " [0.025 0.725 0.575]\n",
      " [0.9   0.3   0.025]\n",
      " [0.675 0.025 0.5  ]\n",
      " [0.75  0.975 0.85 ]\n",
      " [0.125 0.25  0.65 ]\n",
      " [0.375 0.725 0.1  ]\n",
      " [0.75  0.15  0.15 ]\n",
      " [0.9   0.425 0.025]\n",
      " [0.775 0.375 0.325]\n",
      " [0.95  0.725 0.175]\n",
      " [0.4   0.35  0.225]\n",
      " [0.025 0.775 0.975]\n",
      " [0.625 0.65  0.925]\n",
      " [0.375 0.55  0.45 ]\n",
      " [0.35  0.65  0.1  ]\n",
      " [0.35  0.575 0.575]\n",
      " [0.6   0.55  0.2  ]\n",
      " [0.775 0.7   0.725]\n",
      " [0.125 0.875 0.575]\n",
      " [0.225 0.9   0.2  ]\n",
      " [0.525 0.55  0.35 ]\n",
      " [0.7   0.325 0.425]\n",
      " [0.3   0.9   0.9  ]\n",
      " [0.325 0.55  0.525]\n",
      " [0.625 0.8   0.1  ]\n",
      " [0.25  0.575 0.275]\n",
      " [0.075 0.025 0.4  ]\n",
      " [0.325 0.75  0.375]\n",
      " [0.775 0.4   0.7  ]\n",
      " [0.275 0.825 0.2  ]\n",
      " [0.475 0.7   0.65 ]\n",
      " [0.75  0.3   0.925]\n",
      " [0.4   0.2   0.375]\n",
      " [0.85  0.325 0.55 ]\n",
      " [0.125 0.175 0.3  ]\n",
      " [0.15  0.7   0.85 ]\n",
      " [0.55  0.5   0.175]\n",
      " [0.2   0.25  0.4  ]\n",
      " [0.9   0.85  0.475]\n",
      " [0.125 0.525 0.375]\n",
      " [0.85  0.75  0.05 ]\n",
      " [0.575 0.725 0.2  ]\n",
      " [0.725 0.575 0.75 ]\n",
      " [0.675 0.05  0.075]\n",
      " [0.625 0.5   0.025]\n",
      " [0.275 0.675 0.8  ]\n",
      " [0.25  0.6   0.45 ]\n",
      " [0.225 0.825 0.925]\n",
      " [0.05  0.85  0.725]\n",
      " [0.45  0.2   0.375]\n",
      " [0.025 0.    0.65 ]\n",
      " [0.4   0.3   0.85 ]\n",
      " [0.625 0.275 0.35 ]\n",
      " [0.725 0.15  0.625]\n",
      " [0.55  0.175 0.475]\n",
      " [0.425 0.975 0.125]\n",
      " [0.2   0.    0.925]\n",
      " [0.9   0.775 0.225]\n",
      " [0.95  0.775 0.25 ]\n",
      " [0.9   0.15  0.175]\n",
      " [0.325 0.575 0.45 ]\n",
      " [0.5   0.45  0.3  ]\n",
      " [0.175 0.525 0.825]\n",
      " [0.05  0.15  0.125]\n",
      " [0.825 0.8   0.25 ]\n",
      " [0.125 0.225 0.45 ]\n",
      " [0.925 0.775 0.125]\n",
      " [0.025 0.725 0.05 ]\n",
      " [0.775 0.9   0.675]\n",
      " [0.7   0.3   0.2  ]\n",
      " [0.825 0.975 0.9  ]\n",
      " [0.05  0.4   0.675]\n",
      " [0.9   0.5   0.65 ]\n",
      " [0.725 0.2   0.825]\n",
      " [0.375 0.15  0.4  ]\n",
      " [0.15  0.925 0.825]\n",
      " [0.275 0.575 0.95 ]\n",
      " [0.4   0.3   0.15 ]\n",
      " [0.325 0.9   0.825]\n",
      " [0.8   0.35  0.95 ]\n",
      " [0.55  0.225 0.55 ]\n",
      " [0.55  0.175 0.625]\n",
      " [0.4   0.775 0.875]\n",
      " [0.975 0.725 0.45 ]\n",
      " [0.875 0.65  0.325]\n",
      " [0.5   0.15  0.625]\n",
      " [0.6   0.1   0.975]\n",
      " [0.05  0.95  0.775]\n",
      " [0.95  0.925 0.825]\n",
      " [0.2   0.3   0.475]\n",
      " [0.55  0.7   0.325]\n",
      " [0.375 0.05  0.35 ]\n",
      " [0.4   0.225 0.05 ]\n",
      " [0.65  0.475 0.95 ]\n",
      " [0.55  0.    0.8  ]\n",
      " [0.625 0.925 0.275]\n",
      " [0.975 0.2   0.225]\n",
      " [0.725 0.    0.825]\n",
      " [0.775 0.675 0.4  ]\n",
      " [0.05  0.225 0.45 ]\n",
      " [0.7   0.    0.275]\n",
      " [0.875 0.    0.65 ]\n",
      " [0.525 0.85  0.25 ]\n",
      " [0.2   0.075 0.575]\n",
      " [0.875 0.95  0.15 ]\n",
      " [0.775 0.5   0.425]\n",
      " [0.6   0.325 0.175]\n",
      " [0.1   0.375 0.5  ]\n",
      " [0.8   0.075 0.525]\n",
      " [0.775 0.15  0.525]\n",
      " [0.4   0.25  0.525]\n",
      " [0.55  0.2   0.75 ]\n",
      " [0.85  0.7   0.2  ]\n",
      " [0.725 0.35  0.   ]\n",
      " [0.975 0.6   0.5  ]\n",
      " [0.9   0.1   0.7  ]\n",
      " [0.375 0.45  0.325]\n",
      " [0.625 0.3   0.9  ]\n",
      " [0.275 0.375 0.825]\n",
      " [0.5   0.725 0.325]\n",
      " [0.325 0.25  0.175]\n",
      " [0.15  0.125 0.425]\n",
      " [0.725 0.45  0.4  ]\n",
      " [0.975 0.25  0.475]\n",
      " [0.825 0.3   0.45 ]\n",
      " [0.3   0.375 0.3  ]\n",
      " [0.875 0.65  0.55 ]\n",
      " [0.5   0.2   0.425]\n",
      " [0.55  0.4   0.65 ]\n",
      " [0.175 0.825 0.65 ]\n",
      " [0.8   0.825 0.75 ]\n",
      " [0.55  0.55  0.75 ]\n",
      " [0.7   0.35  0.65 ]\n",
      " [0.775 0.3   0.875]\n",
      " [0.125 0.15  0.35 ]\n",
      " [0.825 0.225 0.425]\n",
      " [0.275 0.3   0.15 ]\n",
      " [0.125 0.6   0.275]\n",
      " [0.225 0.45  0.275]\n",
      " [0.575 0.125 0.425]\n",
      " [0.5   0.4   0.875]\n",
      " [0.325 0.6   0.95 ]\n",
      " [0.6   0.125 0.725]\n",
      " [0.3   0.375 0.4  ]\n",
      " [0.675 0.5   0.025]\n",
      " [0.35  0.175 0.05 ]\n",
      " [0.95  0.7   0.275]\n",
      " [0.15  0.125 0.3  ]\n",
      " [0.3   0.15  0.625]\n",
      " [0.525 0.1   0.25 ]\n",
      " [0.2   0.675 0.875]\n",
      " [0.    0.35  0.05 ]\n",
      " [0.625 0.15  0.475]\n",
      " [0.875 0.825 0.575]\n",
      " [0.275 0.475 0.075]\n",
      " [0.95  0.825 0.8  ]\n",
      " [0.95  0.725 0.65 ]\n",
      " [0.4   0.55  0.8  ]\n",
      " [0.725 0.15  0.1  ]\n",
      " [0.075 0.975 0.175]\n",
      " [0.625 0.35  0.95 ]\n",
      " [0.725 0.825 0.275]\n",
      " [0.225 0.7   0.325]\n",
      " [0.05  0.875 0.35 ]\n",
      " [0.175 0.275 0.325]\n",
      " [0.025 0.85  0.325]\n",
      " [0.15  0.675 0.85 ]]\n",
      "Input layer pre-activation: [[ 1.39035598  2.12662995 -0.86217378 ...  3.64044637 -0.51497097\n",
      "   0.31518758]\n",
      " [ 1.33926392  2.52914147 -0.61916328 ...  4.04437172 -0.50325653\n",
      "   0.44315459]\n",
      " [ 1.29576615  2.43040139 -0.90076656 ...  3.62421884 -0.38021512\n",
      "   0.85469975]\n",
      " ...\n",
      " [ 1.34399284  2.06695124 -0.97638265 ...  3.38620248 -0.45102864\n",
      "   0.48618384]\n",
      " [ 1.72981356  1.86264943 -1.07024447 ...  4.06136399 -0.69353179\n",
      "   0.01228889]\n",
      " [ 1.97656314  2.22091986 -1.24597257 ...  4.78347355 -0.70473372\n",
      "   0.51981465]]\n",
      "Input layer output: [[1.39035598 2.12662995 0.         ... 3.64044637 0.         0.31518758]\n",
      " [1.33926392 2.52914147 0.         ... 4.04437172 0.         0.44315459]\n",
      " [1.29576615 2.43040139 0.         ... 3.62421884 0.         0.85469975]\n",
      " ...\n",
      " [1.34399284 2.06695124 0.         ... 3.38620248 0.         0.48618384]\n",
      " [1.72981356 1.86264943 0.         ... 4.06136399 0.         0.01228889]\n",
      " [1.97656314 2.22091986 0.         ... 4.78347355 0.         0.51981465]]\n",
      "Hidden layer pre-activation: [[-2.78308238 -1.08583187 -2.75705596 ... -1.3965625  -2.68498305\n",
      "  -1.51169502]\n",
      " [-3.0877123  -1.1215495  -3.00367131 ... -1.38832014 -3.03571972\n",
      "  -1.62012972]\n",
      " [-2.84096074 -0.7458721  -2.72594749 ... -1.35457661 -2.72010938\n",
      "  -1.45190076]\n",
      " ...\n",
      " [-2.63619838 -0.89809013 -2.59618328 ... -1.36361559 -2.48749406\n",
      "  -1.41647192]\n",
      " [-2.92828061 -1.47714562 -2.99244895 ... -1.59483529 -3.00597549\n",
      "  -1.68181572]\n",
      " [-3.3797871  -1.49201818 -3.3483488  ... -1.75286045 -3.72745369\n",
      "  -1.89161687]]\n",
      "Hidden layer output: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Output layer pre-activation: [[237.23082511 232.473205   224.73701132 203.48820307]\n",
      " [262.99870503 257.72556964 249.20518754 225.59380231]\n",
      " [241.55178608 236.7971179  229.0089302  207.32482987]\n",
      " [300.39551412 294.39950966 284.76353515 257.71228407]\n",
      " [293.58012581 287.75052989 278.34869939 251.90917755]\n",
      " [327.36238438 320.74769609 310.22062535 280.731512  ]\n",
      " [260.10903523 254.81764515 246.31755542 223.00439819]\n",
      " [208.79322012 204.63465791 197.7906972  179.13606036]\n",
      " [215.07554028 210.81078456 203.79296977 184.55379791]\n",
      " [292.98820495 287.08345263 277.62226472 251.27508108]\n",
      " [270.89131144 265.55349329 256.87581941 232.49999751]\n",
      " [221.42723175 217.03088141 209.81574542 189.99618526]\n",
      " [312.7480658  306.44116998 296.37256436 268.21664686]\n",
      " [268.76372689 263.39525099 254.71720576 230.56849451]\n",
      " [273.56843705 268.01464295 259.11105264 234.56084018]\n",
      " [263.06737913 257.8314746  249.34333534 225.7087618 ]\n",
      " [298.28247248 292.2474128  282.6034149  255.78148816]\n",
      " [250.47065191 245.51072916 237.42726102 214.93715401]\n",
      " [253.09102006 248.02971875 239.82296771 217.11402797]\n",
      " [219.47794506 215.17038246 208.05884904 188.39622691]\n",
      " [277.32265699 271.7910042  262.85964969 237.92267257]\n",
      " [354.62584124 347.41191625 336.00729538 304.04202114]\n",
      " [309.64665366 303.4880367  293.59050495 265.68121503]\n",
      " [360.91014355 353.60325827 342.03652709 309.48072845]\n",
      " [287.68924896 281.86083929 272.53452762 246.68617067]\n",
      " [245.14329756 240.15209139 232.10814228 210.16680689]\n",
      " [304.87648048 298.69631591 288.83991856 261.4191766 ]\n",
      " [261.92136903 256.60847466 248.06635141 224.58057143]\n",
      " [244.14746789 239.22258254 231.25020305 209.37987033]\n",
      " [308.93700258 302.7487352  292.83380451 265.00876621]\n",
      " [206.39242699 202.27514196 195.49823803 177.06667745]\n",
      " [265.00205156 259.78718575 251.29330175 227.45520267]\n",
      " [258.53900144 253.31371635 244.89226252 221.70787589]\n",
      " [267.56412025 262.15294776 253.4520944  229.44255472]\n",
      " [255.56444564 250.49704943 242.25361253 219.298907  ]\n",
      " [259.55233529 254.44165399 246.10931283 222.77300049]\n",
      " [366.18878766 358.74824599 346.99592389 313.96841676]\n",
      " [246.79353106 241.89633042 233.91512439 211.76690553]\n",
      " [252.98318602 247.85615131 239.592304   216.92295731]\n",
      " [243.86550015 239.05779554 231.19319382 209.29990886]\n",
      " [264.84661935 259.54751971 250.98068794 227.19505771]\n",
      " [277.78872286 272.23807221 263.28393081 238.30845435]\n",
      " [304.01550978 297.86442104 288.04476514 260.69777735]\n",
      " [238.1821882  233.43213964 225.69065389 204.34297706]\n",
      " [306.32673175 300.15688969 290.29134061 262.72041767]\n",
      " [339.09987667 332.27661607 321.41568534 290.83891213]\n",
      " [270.74252773 265.29399242 256.51973402 232.20730117]\n",
      " [245.51187469 240.55093694 232.52924775 210.53764803]\n",
      " [281.61375265 275.94129387 266.83047316 241.52410599]\n",
      " [266.22309864 260.86758893 252.23318996 228.3339878 ]\n",
      " [336.85410035 330.04713934 319.2290923  288.87075449]\n",
      " [286.37255791 280.55126915 271.24793409 245.52871073]\n",
      " [263.1094364  257.74269274 249.13766133 225.55611316]\n",
      " [273.57380621 268.0694558  259.2097699  234.6373837 ]\n",
      " [277.06400784 271.53778155 262.61452251 237.70114989]\n",
      " [330.2431059  323.55520275 312.9266052  283.1801858 ]\n",
      " [273.45141274 267.94783736 259.09038784 234.52995582]\n",
      " [277.85093529 272.2089586  263.17281087 238.23104613]\n",
      " [247.58285764 242.59498909 234.52317092 212.33534394]\n",
      " [204.39548728 200.39180762 193.74117993 175.46048455]\n",
      " [275.07088925 269.49287251 260.54874822 235.85822717]\n",
      " [312.50864801 306.25274596 296.2325175  268.07833461]\n",
      " [261.402281   256.0837584  247.5431913  224.11201157]\n",
      " [307.08449933 300.87796137 290.97019856 263.33922146]\n",
      " [322.91738512 316.48497797 306.17620437 277.0537915 ]\n",
      " [240.25496087 235.49446274 227.71748281 206.16618748]\n",
      " [299.61956666 293.63984171 284.02815361 257.04771604]\n",
      " [210.57176312 206.41263779 199.54569453 180.71266594]\n",
      " [299.49285778 293.46243372 283.80725853 256.86173146]\n",
      " [256.47338765 251.32447705 242.99705661 219.9867235 ]\n",
      " [230.67529636 226.0958047  218.60089324 197.93258562]\n",
      " [335.67929808 328.86325838 318.05198743 287.81562006]\n",
      " [242.99226752 238.10634445 230.18321804 208.41186625]\n",
      " [287.17195065 281.35439399 272.04427327 246.24312532]\n",
      " [277.21303323 271.59916581 262.59608022 237.7060874 ]\n",
      " [325.94939557 319.38937514 308.92878366 279.55767161]\n",
      " [224.05048044 219.68291597 212.46022115 192.36458453]\n",
      " [249.12268007 244.12261018 236.02047667 213.6833276 ]\n",
      " [302.92702872 296.82548441 287.06364935 259.80431086]\n",
      " [264.77008916 259.43046571 250.82856186 227.06834987]\n",
      " [321.09794916 314.61243446 304.27942883 275.36299902]\n",
      " [291.87922554 285.9654591  276.51027266 250.27841991]\n",
      " [244.12083597 239.28281673 231.3880274  209.48236071]\n",
      " [220.60034204 216.28135045 209.14546506 189.37510941]\n",
      " [289.28212881 283.52913324 274.24975552 248.20841056]\n",
      " [260.99818589 255.8056934  247.38231792 223.93658039]\n",
      " [283.60676687 277.9854018  268.89482801 243.36592462]\n",
      " [258.77280839 253.64544414 245.3081198  222.05740345]\n",
      " [277.56824401 271.89809857 262.84047263 237.93942704]\n",
      " [258.24092271 253.18166782 244.91163921 221.68454409]\n",
      " [308.22505727 301.97822459 292.02020879 264.29230448]\n",
      " [314.28251907 307.91397821 297.77115285 269.488087  ]\n",
      " [257.63530552 252.55818241 244.27991819 221.12147007]\n",
      " [268.72277722 263.30931756 254.59178409 230.46686883]\n",
      " [259.87319688 254.66576303 246.24332007 222.91711495]\n",
      " [286.3104808  280.5768888  271.35233069 245.60097003]\n",
      " [187.58571895 183.89463019 177.73448672 161.00540017]\n",
      " [306.46395592 300.24677238 290.33738603 262.77339481]\n",
      " [227.41353253 222.90439388 215.51328065 195.14180347]\n",
      " [301.391648   295.28280301 285.5338832  258.43195393]\n",
      " [221.70647303 217.21816927 209.91772565 190.11035319]\n",
      " [347.23982361 340.18100491 329.00768954 297.71640436]\n",
      " [255.49360291 250.40750565 242.14833264 219.20892538]\n",
      " [376.37105008 368.72769772 356.6662986  322.70481899]\n",
      " [254.2640649  249.19104393 240.95904221 218.13740902]\n",
      " [325.17466072 318.64354813 308.21845863 278.91251266]\n",
      " [304.83224715 298.77608285 289.03052307 261.55996671]\n",
      " [236.82136084 232.14032714 224.47742069 203.23622838]\n",
      " [313.91665863 307.55018973 297.41383171 269.16660655]\n",
      " [308.69361578 302.50718057 292.59695585 264.79556182]\n",
      " [227.91517891 223.38258247 215.96426618 195.55266741]\n",
      " [325.60026434 318.99935876 308.50681969 279.18866422]\n",
      " [332.66730585 326.02969981 315.41519032 285.40327466]\n",
      " [269.03981769 263.69500131 255.0345075  230.8477131 ]\n",
      " [271.92232862 266.52984199 257.79051661 233.3350592 ]\n",
      " [326.55158755 319.95798727 309.45991985 280.0430163 ]\n",
      " [330.05590107 323.37999409 312.7644316  283.03156617]\n",
      " [305.82784357 299.65525054 289.79356113 262.27395058]\n",
      " [266.21032892 260.93780888 252.37737738 228.44314508]\n",
      " [300.93469596 294.9911121  285.39494179 258.26659788]\n",
      " [303.64775639 297.48156926 287.65313435 260.34969039]\n",
      " [375.76714343 368.13685797 356.09471349 322.18817317]\n",
      " [240.94230566 236.14536188 228.32728094 206.72289527]\n",
      " [284.39190461 278.6383079  269.42021062 243.87030636]\n",
      " [225.05368906 220.63081136 213.34624315 193.17404624]\n",
      " [213.61045831 209.38194653 202.41488436 183.30700758]\n",
      " [330.30127916 323.69052274 313.12979954 283.34378106]\n",
      " [274.34106112 268.93970623 260.16191987 235.46740359]\n",
      " [302.42266518 296.25915171 286.44858041 259.26701562]\n",
      " [271.53567788 266.15913332 257.43887335 233.01527584]\n",
      " [290.06274441 284.3427657  275.08288379 248.94868647]\n",
      " [306.51697808 300.32442062 290.43626501 262.85617288]\n",
      " [221.61471988 217.22186291 210.00746377 190.16754362]\n",
      " [239.8786618  235.18302779 227.46849075 205.9266094 ]\n",
      " [286.31923132 280.67578865 271.53129334 245.73960828]\n",
      " [286.9609545  281.12400673 271.79930782 246.0278372 ]\n",
      " [233.09353114 228.50184832 220.96552698 200.05966444]\n",
      " [312.64023175 306.26760254 296.14190064 268.0255762 ]\n",
      " [295.78569263 289.84606639 280.31850194 253.70559596]\n",
      " [247.4163906  242.48707716 234.46943864 212.27271052]\n",
      " [240.94051594 236.12709092 228.29437519 206.69738076]\n",
      " [275.10085893 269.66729684 260.85126306 236.09452464]\n",
      " [278.70629516 273.18415722 264.24377472 239.16366069]\n",
      " [257.09673028 251.98621884 243.68506893 220.595325  ]\n",
      " [284.72678438 279.0735718  269.94051711 244.31305346]\n",
      " [296.62922169 290.63143359 281.04148079 254.36929929]\n",
      " [243.57961618 238.72581203 230.82382586 208.97942648]\n",
      " [325.20845151 318.65639775 308.21225729 278.91208031]\n",
      " [300.01984065 294.07948142 284.49774317 257.45987703]\n",
      " [252.40009584 247.34227771 239.14735807 216.50629116]\n",
      " [311.06135714 304.86858527 294.92279557 266.88635654]\n",
      " [282.96466434 277.32523447 268.22671145 242.77020568]\n",
      " [282.3721541  276.65363304 267.49226068 242.12987401]\n",
      " [220.61570378 216.24009288 209.05365947 189.30653508]\n",
      " [219.770385   215.43645474 208.29777487 188.61731724]\n",
      " [286.03598165 280.30295452 271.08236858 245.35833166]\n",
      " [297.17112052 291.25749037 281.73553272 254.97209858]\n",
      " [287.0741577  281.35238702 272.12868879 246.29545137]\n",
      " [238.87132271 234.10130972 226.33335778 204.92519936]\n",
      " [325.55212391 318.98184732 308.51715635 279.19043421]\n",
      " [252.36988448 247.36596999 239.21937092 216.55775252]\n",
      " [290.72903665 284.91035375 275.55426793 249.39633648]\n",
      " [293.12150674 287.178784   277.68214245 251.33818412]\n",
      " [350.20975695 343.11102173 331.86466544 300.29254377]\n",
      " [310.57341418 304.33187682 294.34888586 266.38279388]\n",
      " [298.63441277 292.66805745 283.08071239 256.1933744 ]\n",
      " [320.4664333  314.07885977 303.84093988 274.9450851 ]\n",
      " [213.10881194 208.90375794 201.96389882 182.89614364]\n",
      " [279.34419722 273.79395001 264.82050538 239.68861942]\n",
      " [218.25049116 213.91169751 206.78790472 187.26223433]\n",
      " [239.76429449 234.92778326 227.08940411 205.62065175]\n",
      " [236.41929713 231.68241648 223.97492538 202.79855289]\n",
      " [252.63032335 247.63746359 239.49740385 216.80478971]\n",
      " [306.59008052 300.46885114 290.64320641 263.02456169]\n",
      " [314.40545892 308.09801198 298.00796587 269.68581977]\n",
      " [280.86230135 275.30043629 266.29746978 241.01818782]\n",
      " [247.63766952 242.69090829 234.65495566 212.44363652]\n",
      " [252.98855517 247.91096416 239.69102125 216.99950083]\n",
      " [206.05233403 201.98623429 195.25915047 176.83935257]\n",
      " [310.935732   304.65034048 294.62376837 266.64047363]\n",
      " [208.81245149 204.69945653 197.89577752 179.21927078]\n",
      " [250.74682853 245.78439295 237.69519904 215.17845215]\n",
      " [231.57721673 227.01363299 219.52146833 198.75561055]\n",
      " [303.70421498 297.59466163 287.81587976 260.48198453]\n",
      " [191.91408048 188.09351028 181.76350092 164.65632617]\n",
      " [262.72549645 257.52429599 249.07134202 225.45592241]\n",
      " [340.66658276 333.75500083 322.79571837 292.10022972]\n",
      " [224.59860319 220.09525245 212.74486886 192.65359267]\n",
      " [366.19050233 358.77047361 347.03637282 313.99971788]\n",
      " [345.65565715 338.66501423 327.57235506 296.41035388]\n",
      " [303.36022775 297.2713281  287.51526258 260.2068805 ]\n",
      " [237.53418405 232.85935869 225.19296309 203.8774578 ]\n",
      " [255.17470233 249.97191357 241.61299318 218.75667575]\n",
      " [319.1375123  312.77636614 302.57874743 273.80480696]\n",
      " [302.76991701 296.62114318 286.81929098 259.59639853]\n",
      " [259.26371646 254.01400701 245.56167081 222.31518035]\n",
      " [260.84761246 255.52792158 246.99332678 223.61836955]\n",
      " [224.01372329 219.56310791 212.26701719 192.21141202]\n",
      " [254.87696362 249.68266582 241.33506037 218.50493275]\n",
      " [297.64666994 291.65826908 282.06380362 255.28532143]]\n",
      "Output layer output: [[9.91483358e-01 8.51290471e-03 3.72775778e-06 9.91483568e-09]\n",
      " [9.94897607e-01 5.10135589e-03 1.02696980e-06 9.94897603e-09]\n",
      " [9.91458582e-01 8.53785788e-03 3.54973701e-06 9.91458708e-09]\n",
      " [9.97517311e-01 2.48250728e-03 1.72170459e-07 9.97517301e-09]\n",
      " [9.97069078e-01 2.93065963e-03 2.51962572e-07 9.97069069e-09]\n",
      " [9.98661196e-01 1.33874786e-03 4.58661477e-08 9.98661186e-09]\n",
      " [9.94989421e-01 5.00953953e-03 1.02913924e-06 9.94989419e-09]\n",
      " [9.84594344e-01 1.53892335e-02 1.64128103e-05 9.84607315e-09]\n",
      " [9.86127240e-01 1.38603240e-02 1.24256650e-05 9.86132707e-09]\n",
      " [9.97280716e-01 2.71905294e-03 2.21552687e-07 9.97280706e-09]\n",
      " [9.95215804e-01 4.78336141e-03 8.24781034e-07 9.95215796e-09]\n",
      " [9.87818888e-01 1.21721411e-02 8.96090544e-06 9.87821088e-09]\n",
      " [9.98179527e-01 1.82037552e-03 8.71465073e-08 9.98179517e-09]\n",
      " [9.95359580e-01 4.63960992e-03 8.00001414e-07 9.95359573e-09]\n",
      " [9.96141652e-01 3.85780333e-03 5.34236022e-07 9.96141644e-09]\n",
      " [9.94705052e-01 5.29383855e-03 1.09992530e-06 9.94705048e-09]\n",
      " [9.97612177e-01 2.38764874e-03 1.64727152e-07 9.97612167e-09]\n",
      " [9.93033213e-01 6.96461806e-03 2.15920085e-06 9.93033240e-09]\n",
      " [9.93700865e-01 6.29739764e-03 1.72789976e-06 9.93700878e-09]\n",
      " [9.86701877e-01 1.32872654e-02 1.08475280e-05 9.86704998e-09]\n",
      " [9.96055627e-01 3.94383195e-03 5.31250473e-07 9.96055618e-09]\n",
      " [9.99264242e-01 7.35729936e-04 1.81914106e-08 9.99264232e-09]\n",
      " [9.97889152e-01 2.11072176e-03 1.16144905e-07 9.97889142e-09]\n",
      " [9.99329510e-01 6.70463630e-04 1.63466222e-08 9.99329500e-09]\n",
      " [9.97065591e-01 2.93412791e-03 2.71254089e-07 9.97065581e-09]\n",
      " [9.93246250e-01 6.75156213e-03 2.17744152e-06 9.93246305e-09]\n",
      " [9.97934051e-01 2.06582041e-03 1.18250176e-07 9.97934041e-09]\n",
      " [9.95095537e-01 4.90348607e-03 9.66499628e-07 9.95095534e-09]\n",
      " [9.92786321e-01 7.21117187e-03 2.49675057e-06 9.92786390e-09]\n",
      " [9.97950695e-01 2.04918336e-03 1.11272410e-07 9.97950685e-09]\n",
      " [9.83954387e-01 1.60273256e-02 1.82777475e-05 9.83972447e-09]\n",
      " [9.94593088e-01 5.40578519e-03 1.11659784e-06 9.94593083e-09]\n",
      " [9.94648750e-01 5.35005273e-03 1.18746115e-06 9.94648750e-09]\n",
      " [9.95552693e-01 4.44654678e-03 7.50053631e-07 9.95552686e-09]\n",
      " [9.93738962e-01 6.25937242e-03 1.65601741e-06 9.93738969e-09]\n",
      " [9.94002731e-01 5.99580644e-03 1.45257713e-06 9.94002732e-09]\n",
      " [9.99413343e-01 5.86632810e-04 1.46114569e-08 9.99413333e-09]\n",
      " [9.92585347e-01 7.41209921e-03 2.54357750e-06 9.92585398e-09]\n",
      " [9.94099335e-01 5.89912538e-03 1.52994106e-06 9.94099347e-09]\n",
      " [9.91896454e-01 8.10041524e-03 3.12130301e-06 9.91896540e-09]\n",
      " [9.95027776e-01 4.97125841e-03 9.56051717e-07 9.95027770e-09]\n",
      " [9.96129578e-01 3.86990229e-03 5.09955170e-07 9.96129569e-09]\n",
      " [9.97873227e-01 2.12663770e-03 1.25608352e-07 9.97873217e-09]\n",
      " [9.91419204e-01 8.57705006e-03 3.73600041e-06 9.91419394e-09]\n",
      " [9.97912661e-01 2.08721046e-03 1.18374473e-07 9.97912651e-09]\n",
      " [9.98912965e-01 1.08699416e-03 3.08523302e-08 9.98912955e-09]\n",
      " [9.95715142e-01 4.28417594e-03 6.72561503e-07 9.95715133e-09]\n",
      " [9.93040095e-01 6.95760084e-03 2.29386435e-06 9.93040150e-09]\n",
      " [9.96571984e-01 3.42761776e-03 3.88593053e-07 9.96571974e-09]\n",
      " [9.95299269e-01 4.69987477e-03 8.45966974e-07 9.95299263e-09]\n",
      " [9.98895121e-01 1.10483700e-03 3.21237791e-08 9.98895111e-09]\n",
      " [9.97044677e-01 2.95503410e-03 2.79231754e-07 9.97044667e-09]\n",
      " [9.95351513e-01 4.64761609e-03 8.61310415e-07 9.95351508e-09]\n",
      " [9.95946856e-01 4.05254849e-03 5.85418557e-07 9.95946847e-09]\n",
      " [9.96034246e-01 3.96520617e-03 5.38335660e-07 9.96034236e-09]\n",
      " [9.98755598e-01 1.24435207e-03 4.01175136e-08 9.98755588e-09]\n",
      " [9.95943725e-01 4.05567780e-03 5.87152270e-07 9.95943716e-09]\n",
      " [9.96466227e-01 3.53333209e-03 4.30530641e-07 9.96466218e-09]\n",
      " [9.93223884e-01 6.77398139e-03 2.12486844e-06 9.93223923e-09]\n",
      " [9.82055879e-01 1.79209261e-02 2.31853307e-05 9.82082527e-09]\n",
      " [9.96233661e-01 3.76582785e-03 5.01407984e-07 9.96233652e-09]\n",
      " [9.98084467e-01 1.91542793e-03 9.51992940e-08 9.98084457e-09]\n",
      " [9.95122927e-01 4.87609993e-03 9.62638902e-07 9.95122924e-09]\n",
      " [9.97987721e-01 2.01215838e-03 1.10158093e-07 9.97987712e-09]\n",
      " [9.98393926e-01 1.60600056e-03 6.35265448e-08 9.98393916e-09]\n",
      " [9.91507772e-01 8.48864863e-03 3.56900187e-06 9.91507918e-09]\n",
      " [9.97476662e-01 2.52314905e-03 1.78878049e-07 9.97476652e-09]\n",
      " [9.84603250e-01 1.53807090e-02 1.60312479e-05 9.84613848e-09]\n",
      " [9.97601110e-01 2.39871600e-03 1.63716309e-07 9.97601100e-09]\n",
      " [9.94226369e-01 5.77221583e-03 1.40563258e-06 9.94226373e-09]\n",
      " [9.89838473e-01 1.01558620e-02 5.65559681e-06 9.89839059e-09]\n",
      " [9.98905095e-01 1.09486294e-03 3.20731902e-08 9.98905085e-09]\n",
      " [9.92501734e-01 7.49553120e-03 2.72531216e-06 9.92501819e-09]\n",
      " [9.97033661e-01 2.96605036e-03 2.78407725e-07 9.97033651e-09]\n",
      " [9.96365839e-01 3.63369415e-03 4.57014366e-07 9.96365830e-09]\n",
      " [9.98586075e-01 1.41386484e-03 5.04833099e-08 9.98586065e-09]\n",
      " [9.87467676e-01 1.25231644e-02 9.14968603e-06 9.87469378e-09]\n",
      " [9.93305571e-01 6.69238236e-03 2.03700063e-06 9.93305601e-09]\n",
      " [9.97765439e-01 2.23441258e-03 1.38698929e-07 9.97765429e-09]\n",
      " [9.95224328e-01 4.77477470e-03 8.87342084e-07 9.95224322e-09]\n",
      " [9.98476868e-01 1.52306207e-03 5.95466094e-08 9.98476858e-09]\n",
      " [9.97305050e-01 2.69471902e-03 2.20921618e-07 9.97305040e-09]\n",
      " [9.92136609e-01 7.86044181e-03 2.93935030e-06 9.92136689e-09]\n",
      " [9.86851263e-01 1.31382588e-02 1.04681863e-05 9.86853965e-09]\n",
      " [9.96836453e-01 3.16323202e-03 3.05189290e-07 9.96836443e-09]\n",
      " [9.94471349e-01 5.52741681e-03 1.22416070e-06 9.94471347e-09]\n",
      " [9.96392924e-01 3.60665000e-03 4.16516550e-07 9.96392914e-09]\n",
      " [9.94101374e-01 5.89719377e-03 1.42179801e-06 9.94101376e-09]\n",
      " [9.96564051e-01 3.43552884e-03 4.10200922e-07 9.96564042e-09]\n",
      " [9.93688147e-01 6.31021722e-03 1.62584192e-06 9.93688150e-09]\n",
      " [9.98067044e-01 1.93284484e-03 1.01493806e-07 9.98067034e-09]\n",
      " [9.98288179e-01 1.71173411e-03 7.73519720e-08 9.98288169e-09]\n",
      " [9.93799246e-01 6.19915939e-03 1.58438351e-06 9.93799250e-09]\n",
      " [9.95562820e-01 4.43643356e-03 7.36155760e-07 9.95562813e-09]\n",
      " [9.94552892e-01 5.44589026e-03 1.20736852e-06 9.94552891e-09]\n",
      " [9.96774653e-01 3.22500921e-03 3.27914891e-07 9.96774643e-09]\n",
      " [9.75612097e-01 2.43364858e-02 5.14071824e-05 9.75891079e-09]\n",
      " [9.98008987e-01 1.99089393e-03 1.08938825e-07 9.98008977e-09]\n",
      " [9.89105247e-01 1.08880189e-02 6.72462449e-06 9.89106191e-09]\n",
      " [9.97781655e-01 2.21819537e-03 1.39425941e-07 9.97781645e-09]\n",
      " [9.88877783e-01 1.11146924e-02 7.51495761e-06 9.88879648e-09]\n",
      " [9.99140903e-01 8.59064504e-04 2.20559697e-08 9.99140893e-09]\n",
      " [9.93854273e-01 6.14411635e-03 1.60048172e-06 9.93854281e-09]\n",
      " [9.99520978e-01 4.78999671e-04 1.27629610e-08 9.99520968e-09]\n",
      " [9.93773842e-01 6.22448228e-03 1.66566813e-06 9.93773852e-09]\n",
      " [9.98544664e-01 1.45527247e-03 5.31753754e-08 9.98544654e-09]\n",
      " [9.97661940e-01 2.33790315e-03 1.46869804e-07 9.97661930e-09]\n",
      " [9.90811400e-01 9.18426391e-03 4.32595245e-06 9.90811647e-09]\n",
      " [9.98284634e-01 1.71527831e-03 7.79294438e-08 9.98284624e-09]\n",
      " [9.97946945e-01 2.05293353e-03 1.11936427e-07 9.97946935e-09]\n",
      " [9.89355324e-01 1.06382715e-02 6.39453982e-06 9.89356186e-09]\n",
      " [9.98642641e-01 1.35730182e-03 4.76413189e-08 9.98642631e-09]\n",
      " [9.98691492e-01 1.30845559e-03 4.21185459e-08 9.98691482e-09]\n",
      " [9.95248995e-01 4.75016169e-03 8.33147657e-07 9.95248988e-09]\n",
      " [9.95469219e-01 4.53003543e-03 7.35492183e-07 9.95469211e-09]\n",
      " [9.98632702e-01 1.36723993e-03 4.77078143e-08 9.98632692e-09]\n",
      " [9.98740599e-01 1.25935039e-03 4.08806079e-08 9.98740589e-09]\n",
      " [9.97918383e-01 2.08148846e-03 1.18495397e-07 9.97918373e-09]\n",
      " [9.94894522e-01 5.10448016e-03 9.87641954e-07 9.94894516e-09]\n",
      " [9.97384039e-01 2.61576301e-03 1.87815264e-07 9.97384029e-09]\n",
      " [9.97905030e-01 2.09483697e-03 1.22884034e-07 9.97905020e-09]\n",
      " [9.99514681e-01 4.85296597e-04 1.28537994e-08 9.99514671e-09]\n",
      " [9.91809352e-01 8.18733396e-03 3.30444131e-06 9.91809478e-09]\n",
      " [9.96838329e-01 3.16133698e-03 3.23658359e-07 9.96838320e-09]\n",
      " [9.88134566e-01 1.18572798e-02 8.14449496e-06 9.88135967e-09]\n",
      " [9.85621917e-01 1.43645263e-02 1.35472280e-05 9.85628716e-09]\n",
      " [9.98655930e-01 1.34401487e-03 4.48152411e-08 9.98655920e-09]\n",
      " [9.95509069e-01 4.49021933e-03 7.01981094e-07 9.95509060e-09]\n",
      " [9.97899431e-01 2.10043354e-03 1.25226066e-07 9.97899421e-09]\n",
      " [9.95396732e-01 4.60249696e-03 7.61285789e-07 9.95396724e-09]\n",
      " [9.96730601e-01 3.26906774e-03 3.21072297e-07 9.96730591e-09]\n",
      " [9.97959447e-01 2.04042898e-03 1.13576776e-07 9.97959437e-09]\n",
      " [9.87776775e-01 1.22142166e-02 8.99846688e-06 9.87778940e-09]\n",
      " [9.90943588e-01 9.05235259e-03 4.04989784e-06 9.90943756e-09]\n",
      " [9.96471429e-01 3.52817453e-03 3.86794291e-07 9.96471419e-09]\n",
      " [9.97090465e-01 2.90925533e-03 2.69457583e-07 9.97090455e-09]\n",
      " [9.89960589e-01 1.00340398e-02 5.36161014e-06 9.89961025e-09]\n",
      " [9.98295150e-01 1.70476217e-03 7.82364302e-08 9.98295140e-09]\n",
      " [9.97373680e-01 2.62610840e-03 2.01197706e-07 9.97373671e-09]\n",
      " [9.92818073e-01 7.17954043e-03 2.37628307e-06 9.92818118e-09]\n",
      " [9.91942205e-01 8.05458142e-03 3.20393640e-06 9.91942328e-09]\n",
      " [9.95650810e-01 4.34852536e-03 6.54995833e-07 9.95650801e-09]\n",
      " [9.96018073e-01 3.98138583e-03 5.31484295e-07 9.96018063e-09]\n",
      " [9.94001673e-01 5.99681863e-03 1.49853514e-06 9.94001677e-09]\n",
      " [9.96505610e-01 3.49399238e-03 3.87437679e-07 9.96505601e-09]\n",
      " [9.97521716e-01 2.47809430e-03 1.79507533e-07 9.97521706e-09]\n",
      " [9.92258824e-01 7.73829288e-03 2.87314849e-06 9.92258907e-09]\n",
      " [9.98574781e-01 1.42515742e-03 5.14837531e-08 9.98574771e-09]\n",
      " [9.97375610e-01 2.62418928e-03 1.90981626e-07 9.97375600e-09]\n",
      " [9.93679004e-01 6.31923185e-03 1.75437354e-06 9.93679020e-09]\n",
      " [9.97959890e-01 2.03999243e-03 1.07753928e-07 9.97959880e-09]\n",
      " [9.96457273e-01 3.54231048e-03 4.06103078e-07 9.96457264e-09]\n",
      " [9.96725816e-01 3.27382064e-03 3.53778521e-07 9.96725806e-09]\n",
      " [9.87566532e-01 1.24240457e-02 9.41218033e-06 9.87569018e-09]\n",
      " [9.87043730e-01 1.29459737e-02 1.02863678e-05 9.87046636e-09]\n",
      " [9.96772835e-01 3.22682554e-03 3.29360109e-07 9.96772825e-09]\n",
      " [9.97304697e-01 2.69508536e-03 2.07323104e-07 9.97304687e-09]\n",
      " [9.96736424e-01 3.26323402e-03 3.31959785e-07 9.96736414e-09]\n",
      " [9.91587483e-01 8.40893946e-03 3.56755628e-06 9.91587653e-09]\n",
      " [9.98600482e-01 1.39945842e-03 4.99068160e-08 9.98600472e-09]\n",
      " [9.93331174e-01 6.66687418e-03 1.94145055e-06 9.93331192e-09]\n",
      " [9.97037002e-01 2.96272169e-03 2.66060565e-07 9.97036992e-09]\n",
      " [9.97381773e-01 2.61801051e-03 2.06595183e-07 9.97381763e-09]\n",
      " [9.99174492e-01 8.25477151e-04 2.07680361e-08 9.99174482e-09]\n",
      " [9.98056803e-01 1.94308702e-03 9.97094250e-08 9.98056793e-09]\n",
      " [9.97442783e-01 2.55702215e-03 1.85363181e-07 9.97442773e-09]\n",
      " [9.98320402e-01 1.67951771e-03 7.00881054e-08 9.98320392e-09]\n",
      " [9.85285297e-01 1.47004471e-02 1.42458447e-05 9.85292741e-09]\n",
      " [9.96128032e-01 3.87145788e-03 5.00593351e-07 9.96128022e-09]\n",
      " [9.87105626e-01 1.28839733e-02 1.03905458e-05 9.87109055e-09]\n",
      " [9.92124666e-01 7.87220968e-03 3.11398971e-06 9.92124804e-09]\n",
      " [9.91306325e-01 8.68974916e-03 3.91549623e-06 9.91306564e-09]\n",
      " [9.93257531e-01 6.74048367e-03 1.97558801e-06 9.93257548e-09]\n",
      " [9.97808908e-01 2.19095375e-03 1.28393423e-07 9.97808898e-09]\n",
      " [9.98180530e-01 1.81937438e-03 8.54681409e-08 9.98180520e-09]\n",
      " [9.96172598e-01 3.82691145e-03 4.80839949e-07 9.96172588e-09]\n",
      " [9.92941460e-01 7.05623677e-03 2.29343800e-06 9.92941502e-09]\n",
      " [9.93802035e-01 6.19627687e-03 1.67816166e-06 9.93802048e-09]\n",
      " [9.83124961e-01 1.68548268e-02 2.02022645e-05 9.83145162e-09]\n",
      " [9.98140026e-01 1.85987153e-03 9.22048458e-08 9.98140016e-09]\n",
      " [9.83886991e-01 1.60951291e-02 1.78703528e-05 9.83900810e-09]\n",
      " [9.93050588e-01 6.94726075e-03 2.14160408e-06 9.93050613e-09]\n",
      " [9.89677198e-01 1.03170311e-02 5.76096543e-06 9.89677739e-09]\n",
      " [9.97783226e-01 2.21662815e-03 1.35528747e-07 9.97783216e-09]\n",
      " [9.78517256e-01 2.14445100e-02 3.82242510e-05 9.78659373e-09]\n",
      " [9.94519055e-01 5.47975613e-03 1.17860789e-06 9.94519052e-09]\n",
      " [9.99004763e-01 9.95199209e-04 2.73021480e-08 9.99004754e-09]\n",
      " [9.89042421e-01 1.09505248e-02 7.04390888e-06 9.89043735e-09]\n",
      " [9.99401192e-01 5.98783161e-04 1.47936346e-08 9.99401182e-09]\n",
      " [9.99080348e-01 9.19618050e-04 2.39906120e-08 9.99080338e-09]\n",
      " [9.97737068e-01 2.26278087e-03 1.41087172e-07 9.97737058e-09]\n",
      " [9.90754721e-01 9.24093157e-03 4.33745643e-06 9.90754951e-09]\n",
      " [9.94527592e-01 5.47110653e-03 1.29180760e-06 9.94527597e-09]\n",
      " [9.98275499e-01 1.72441680e-03 7.42323213e-08 9.98275489e-09]\n",
      " [9.97868306e-01 2.13155583e-03 1.27957587e-07 9.97868296e-09]\n",
      " [9.94777227e-01 5.22163860e-03 1.12425004e-06 9.94777226e-09]\n",
      " [9.95128589e-01 4.87043430e-03 9.67232148e-07 9.95128585e-09]\n",
      " [9.88455504e-01 1.15366519e-02 7.83385996e-06 9.88457020e-09]\n",
      " [9.94481171e-01 5.51750167e-03 1.31738764e-06 9.94481177e-09]\n",
      " [9.97498402e-01 2.50140772e-03 1.80331735e-07 9.97498392e-09]]\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "y_pred = model.forward(X_train_small_scaled)\n",
    "loss = loss_fn.forward(y_pred, Y_train_small)\n",
    "loss_grad = loss_fn.backward(y_pred, Y_train_small)\n",
    "model.backward(loss_grad, Y_train_small)\n",
    "optimizer.step()\n",
    "count += 1\n",
    "\n",
    "print(count, loss, sep='\\t\\t')\n",
    "# print(loss_grad)\n",
    "print_model_details(model)\n",
    "print_intermediates(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1.4722747927766842\n",
      "1\t1.4460252096305346\n",
      "2\t1.4219284175769242\n",
      "3\t1.3993841735972916\n",
      "4\t1.3774474002763082\n",
      "5\t1.357204827009818\n",
      "6\t1.3381384885542378\n",
      "7\t1.3199799216711465\n",
      "8\t1.3025982313445212\n",
      "9\t1.2883038260886466\n",
      "10\t1.2742314713959493\n",
      "11\t1.260261790643795\n",
      "12\t1.2464226071943643\n",
      "13\t1.2327538883659643\n",
      "14\t1.2193092161053192\n",
      "15\t1.2061573166054977\n",
      "16\t1.1933834382315989\n",
      "17\t1.1810902109996342\n",
      "18\t1.1693973864552287\n",
      "19\t1.1584395295628118\n",
      "20\t1.148360326213737\n",
      "21\t1.1393017654432072\n",
      "22\t1.131386295682727\n",
      "23\t1.1246906641917926\n",
      "24\t1.1191896553702279\n",
      "25\t1.1146971479548586\n",
      "26\t1.1110797164740154\n",
      "27\t1.1079763230329986\n",
      "28\t1.1049912024049524\n",
      "29\t1.1018481507038957\n",
      "30\t1.0986110627816645\n",
      "31\t1.0959181678952574\n",
      "32\t1.0947879005933903\n",
      "33\t1.0949152082600284\n",
      "34\t1.0944070860952595\n",
      "35\t1.0958142575420555\n",
      "36\t1.115296995383798\n",
      "37\t7.801590352913693\n",
      "38\t7.9731983188085165\n",
      "39\t12.280453865968243\n",
      "40\t12.28045386596824\n",
      "41\t12.28045386596824\n",
      "42\t12.280453865968243\n",
      "43\t12.28045386596824\n",
      "44\t12.28045386596824\n",
      "45\t12.28045386596824\n",
      "46\t12.280453865968243\n",
      "47\t12.28045386596824\n",
      "48\t12.28045386596824\n",
      "49\t12.28045386596824\n",
      "50\tnan\n",
      "51\tnan\n",
      "52\tnan\n",
      "53\tnan\n",
      "54\tnan\n",
      "55\tnan\n",
      "56\tnan\n",
      "57\tnan\n",
      "58\tnan\n",
      "59\tnan\n",
      "60\tnan\n",
      "61\tnan\n",
      "62\tnan\n",
      "63\tnan\n",
      "64\tnan\n",
      "65\tnan\n",
      "66\tnan\n",
      "67\tnan\n",
      "68\tnan\n",
      "69\tnan\n",
      "70\tnan\n",
      "71\tnan\n",
      "72\tnan\n",
      "73\tnan\n",
      "74\tnan\n",
      "75\tnan\n",
      "76\tnan\n",
      "77\tnan\n",
      "78\tnan\n",
      "79\tnan\n",
      "80\tnan\n",
      "81\tnan\n",
      "82\tnan\n",
      "83\tnan\n",
      "84\tnan\n",
      "85\tnan\n",
      "86\tnan\n",
      "87\tnan\n",
      "88\tnan\n",
      "89\tnan\n",
      "90\tnan\n",
      "91\tnan\n",
      "92\tnan\n",
      "93\tnan\n",
      "94\tnan\n",
      "95\tnan\n",
      "96\tnan\n",
      "97\tnan\n",
      "98\tnan\n",
      "99\tnan\n",
      "100\tnan\n",
      "101\tnan\n",
      "102\tnan\n",
      "103\tnan\n",
      "104\tnan\n",
      "105\tnan\n",
      "106\tnan\n",
      "107\tnan\n",
      "108\tnan\n",
      "109\tnan\n",
      "110\tnan\n",
      "111\tnan\n",
      "112\tnan\n",
      "113\tnan\n",
      "114\tnan\n",
      "115\tnan\n",
      "116\tnan\n",
      "117\tnan\n",
      "118\tnan\n",
      "119\tnan\n",
      "120\tnan\n",
      "121\tnan\n",
      "122\tnan\n",
      "123\tnan\n",
      "124\tnan\n",
      "125\tnan\n",
      "126\tnan\n",
      "127\tnan\n",
      "128\tnan\n",
      "129\tnan\n",
      "130\tnan\n",
      "131\tnan\n",
      "132\tnan\n",
      "133\tnan\n",
      "134\tnan\n",
      "135\tnan\n",
      "136\tnan\n",
      "137\tnan\n",
      "138\tnan\n",
      "139\tnan\n",
      "140\tnan\n",
      "141\tnan\n",
      "142\tnan\n",
      "143\tnan\n",
      "144\tnan\n",
      "145\tnan\n",
      "146\tnan\n",
      "147\tnan\n",
      "148\tnan\n",
      "149\tnan\n",
      "150\tnan\n",
      "151\tnan\n",
      "152\tnan\n",
      "153\tnan\n",
      "154\tnan\n",
      "155\tnan\n",
      "156\tnan\n",
      "157\tnan\n",
      "158\tnan\n",
      "159\tnan\n",
      "160\tnan\n",
      "161\tnan\n",
      "162\tnan\n",
      "163\tnan\n",
      "164\tnan\n",
      "165\tnan\n",
      "166\tnan\n",
      "167\tnan\n",
      "168\tnan\n",
      "169\tnan\n",
      "170\tnan\n",
      "171\tnan\n",
      "172\tnan\n",
      "173\tnan\n",
      "174\tnan\n",
      "175\tnan\n",
      "176\tnan\n",
      "177\tnan\n",
      "178\tnan\n",
      "179\tnan\n",
      "180\tnan\n",
      "181\tnan\n",
      "182\tnan\n",
      "183\tnan\n",
      "184\tnan\n",
      "185\tnan\n",
      "186\tnan\n",
      "187\tnan\n",
      "188\tnan\n",
      "189\tnan\n",
      "190\tnan\n",
      "191\tnan\n",
      "192\tnan\n",
      "193\tnan\n",
      "194\tnan\n",
      "195\tnan\n",
      "196\tnan\n",
      "197\tnan\n",
      "198\tnan\n",
      "199\tnan\n",
      "200\tnan\n",
      "201\tnan\n",
      "202\tnan\n",
      "203\tnan\n",
      "204\tnan\n",
      "205\tnan\n",
      "206\tnan\n",
      "207\tnan\n",
      "208\tnan\n",
      "209\tnan\n",
      "210\tnan\n",
      "211\tnan\n",
      "212\tnan\n",
      "213\tnan\n",
      "214\tnan\n",
      "215\tnan\n",
      "216\tnan\n",
      "217\tnan\n",
      "218\tnan\n",
      "219\tnan\n",
      "220\tnan\n",
      "221\tnan\n",
      "222\tnan\n",
      "223\tnan\n",
      "224\tnan\n",
      "225\tnan\n",
      "226\tnan\n",
      "227\tnan\n",
      "228\tnan\n",
      "229\tnan\n",
      "230\tnan\n",
      "231\tnan\n",
      "232\tnan\n",
      "233\tnan\n",
      "234\tnan\n",
      "235\tnan\n",
      "236\tnan\n",
      "237\tnan\n",
      "238\tnan\n",
      "239\tnan\n",
      "240\tnan\n",
      "241\tnan\n",
      "242\tnan\n",
      "243\tnan\n",
      "244\tnan\n",
      "245\tnan\n",
      "246\tnan\n",
      "247\tnan\n",
      "248\tnan\n",
      "249\tnan\n",
      "250\tnan\n",
      "251\tnan\n",
      "252\tnan\n",
      "253\tnan\n",
      "254\tnan\n",
      "255\tnan\n",
      "256\tnan\n",
      "257\tnan\n",
      "258\tnan\n",
      "259\tnan\n",
      "260\tnan\n",
      "261\tnan\n",
      "262\tnan\n",
      "263\tnan\n",
      "264\tnan\n",
      "265\tnan\n",
      "266\tnan\n",
      "267\tnan\n",
      "268\tnan\n",
      "269\tnan\n",
      "270\tnan\n",
      "271\tnan\n",
      "272\tnan\n",
      "273\tnan\n",
      "274\tnan\n",
      "275\tnan\n",
      "276\tnan\n",
      "277\tnan\n",
      "278\tnan\n",
      "279\tnan\n",
      "280\tnan\n",
      "281\tnan\n",
      "282\tnan\n",
      "283\tnan\n",
      "284\tnan\n",
      "285\tnan\n",
      "286\tnan\n",
      "287\tnan\n",
      "288\tnan\n",
      "289\tnan\n",
      "290\tnan\n",
      "291\tnan\n",
      "292\tnan\n",
      "293\tnan\n",
      "294\tnan\n",
      "295\tnan\n",
      "296\tnan\n",
      "297\tnan\n",
      "298\tnan\n",
      "299\tnan\n",
      "300\tnan\n",
      "301\tnan\n",
      "302\tnan\n",
      "303\tnan\n",
      "304\tnan\n",
      "305\tnan\n",
      "306\tnan\n",
      "307\tnan\n",
      "308\tnan\n",
      "309\tnan\n",
      "310\tnan\n",
      "311\tnan\n",
      "312\tnan\n",
      "313\tnan\n",
      "314\tnan\n",
      "315\tnan\n",
      "316\tnan\n",
      "317\tnan\n",
      "318\tnan\n",
      "319\tnan\n",
      "320\tnan\n",
      "321\tnan\n",
      "322\tnan\n",
      "323\tnan\n",
      "324\tnan\n",
      "325\tnan\n",
      "326\tnan\n",
      "327\tnan\n",
      "328\tnan\n",
      "329\tnan\n",
      "330\tnan\n",
      "331\tnan\n",
      "332\tnan\n",
      "333\tnan\n",
      "334\tnan\n",
      "335\tnan\n",
      "336\tnan\n",
      "337\tnan\n",
      "338\tnan\n",
      "339\tnan\n",
      "340\tnan\n",
      "341\tnan\n",
      "342\tnan\n",
      "343\tnan\n",
      "344\tnan\n",
      "345\tnan\n",
      "346\tnan\n",
      "347\tnan\n",
      "348\tnan\n",
      "349\tnan\n",
      "350\tnan\n",
      "351\tnan\n",
      "352\tnan\n",
      "353\tnan\n",
      "354\tnan\n",
      "355\tnan\n",
      "356\tnan\n",
      "357\tnan\n",
      "358\tnan\n",
      "359\tnan\n",
      "360\tnan\n",
      "361\tnan\n",
      "362\tnan\n",
      "363\tnan\n",
      "364\tnan\n",
      "365\tnan\n",
      "366\tnan\n",
      "367\tnan\n",
      "368\tnan\n",
      "369\tnan\n",
      "370\tnan\n",
      "371\tnan\n",
      "372\tnan\n",
      "373\tnan\n",
      "374\tnan\n",
      "375\tnan\n",
      "376\tnan\n",
      "377\tnan\n",
      "378\tnan\n",
      "379\tnan\n",
      "380\tnan\n",
      "381\tnan\n",
      "382\tnan\n",
      "383\tnan\n",
      "384\tnan\n",
      "385\tnan\n",
      "386\tnan\n",
      "387\tnan\n",
      "388\tnan\n",
      "389\tnan\n",
      "390\tnan\n",
      "391\tnan\n",
      "392\tnan\n",
      "393\tnan\n",
      "394\tnan\n",
      "395\tnan\n",
      "396\tnan\n",
      "397\tnan\n",
      "398\tnan\n",
      "399\tnan\n",
      "400\tnan\n",
      "401\tnan\n",
      "402\tnan\n",
      "403\tnan\n",
      "404\tnan\n",
      "405\tnan\n",
      "406\tnan\n",
      "407\tnan\n",
      "408\tnan\n",
      "409\tnan\n",
      "410\tnan\n",
      "411\tnan\n",
      "412\tnan\n",
      "413\tnan\n",
      "414\tnan\n",
      "415\tnan\n",
      "416\tnan\n",
      "417\tnan\n",
      "418\tnan\n",
      "419\tnan\n",
      "420\tnan\n",
      "421\tnan\n",
      "422\tnan\n",
      "423\tnan\n",
      "424\tnan\n",
      "425\tnan\n",
      "426\tnan\n",
      "427\tnan\n",
      "428\tnan\n",
      "429\tnan\n",
      "430\tnan\n",
      "431\tnan\n",
      "432\tnan\n",
      "433\tnan\n",
      "434\tnan\n",
      "435\tnan\n",
      "436\tnan\n",
      "437\tnan\n",
      "438\tnan\n",
      "439\tnan\n",
      "440\tnan\n",
      "441\tnan\n",
      "442\tnan\n",
      "443\tnan\n",
      "444\tnan\n",
      "445\tnan\n",
      "446\tnan\n",
      "447\tnan\n",
      "448\tnan\n",
      "449\tnan\n",
      "450\tnan\n",
      "451\tnan\n",
      "452\tnan\n",
      "453\tnan\n",
      "454\tnan\n",
      "455\tnan\n",
      "456\tnan\n",
      "457\tnan\n",
      "458\tnan\n",
      "459\tnan\n",
      "460\tnan\n",
      "461\tnan\n",
      "462\tnan\n",
      "463\tnan\n",
      "464\tnan\n",
      "465\tnan\n",
      "466\tnan\n",
      "467\tnan\n",
      "468\tnan\n",
      "469\tnan\n",
      "470\tnan\n",
      "471\tnan\n",
      "472\tnan\n",
      "473\tnan\n",
      "474\tnan\n",
      "475\tnan\n",
      "476\tnan\n",
      "477\tnan\n",
      "478\tnan\n",
      "479\tnan\n",
      "480\tnan\n",
      "481\tnan\n",
      "482\tnan\n",
      "483\tnan\n",
      "484\tnan\n",
      "485\tnan\n",
      "486\tnan\n",
      "487\tnan\n",
      "488\tnan\n",
      "489\tnan\n",
      "490\tnan\n",
      "491\tnan\n",
      "492\tnan\n",
      "493\tnan\n",
      "494\tnan\n",
      "495\tnan\n",
      "496\tnan\n",
      "497\tnan\n",
      "498\tnan\n",
      "499\tnan\n",
      "500\tnan\n",
      "501\tnan\n",
      "502\tnan\n",
      "503\tnan\n",
      "504\tnan\n",
      "505\tnan\n",
      "506\tnan\n",
      "507\tnan\n",
      "508\tnan\n",
      "509\tnan\n",
      "510\tnan\n",
      "511\tnan\n",
      "512\tnan\n",
      "513\tnan\n",
      "514\tnan\n",
      "515\tnan\n",
      "516\tnan\n",
      "517\tnan\n",
      "518\tnan\n",
      "519\tnan\n",
      "520\tnan\n",
      "521\tnan\n",
      "522\tnan\n",
      "523\tnan\n",
      "524\tnan\n",
      "525\tnan\n",
      "526\tnan\n",
      "527\tnan\n",
      "528\tnan\n",
      "529\tnan\n",
      "530\tnan\n",
      "531\tnan\n",
      "532\tnan\n",
      "533\tnan\n",
      "534\tnan\n",
      "535\tnan\n",
      "536\tnan\n",
      "537\tnan\n",
      "538\tnan\n",
      "539\tnan\n",
      "540\tnan\n",
      "541\tnan\n",
      "542\tnan\n",
      "543\tnan\n",
      "544\tnan\n",
      "545\tnan\n",
      "546\tnan\n",
      "547\tnan\n",
      "548\tnan\n",
      "549\tnan\n",
      "550\tnan\n",
      "551\tnan\n",
      "552\tnan\n",
      "553\tnan\n",
      "554\tnan\n",
      "555\tnan\n",
      "556\tnan\n",
      "557\tnan\n",
      "558\tnan\n",
      "559\tnan\n",
      "560\tnan\n",
      "561\tnan\n",
      "562\tnan\n",
      "563\tnan\n",
      "564\tnan\n",
      "565\tnan\n",
      "566\tnan\n",
      "567\tnan\n",
      "568\tnan\n",
      "569\tnan\n",
      "570\tnan\n",
      "571\tnan\n",
      "572\tnan\n",
      "573\tnan\n",
      "574\tnan\n",
      "575\tnan\n",
      "576\tnan\n",
      "577\tnan\n",
      "578\tnan\n",
      "579\tnan\n",
      "580\tnan\n",
      "581\tnan\n",
      "582\tnan\n",
      "583\tnan\n",
      "584\tnan\n",
      "585\tnan\n",
      "586\tnan\n",
      "587\tnan\n",
      "588\tnan\n",
      "589\tnan\n",
      "590\tnan\n",
      "591\tnan\n",
      "592\tnan\n",
      "593\tnan\n",
      "594\tnan\n",
      "595\tnan\n",
      "596\tnan\n",
      "597\tnan\n",
      "598\tnan\n",
      "599\tnan\n",
      "600\tnan\n",
      "601\tnan\n",
      "602\tnan\n",
      "603\tnan\n",
      "604\tnan\n",
      "605\tnan\n",
      "606\tnan\n",
      "607\tnan\n",
      "608\tnan\n",
      "609\tnan\n",
      "610\tnan\n",
      "611\tnan\n",
      "612\tnan\n",
      "613\tnan\n",
      "614\tnan\n",
      "615\tnan\n",
      "616\tnan\n",
      "617\tnan\n",
      "618\tnan\n",
      "619\tnan\n",
      "620\tnan\n",
      "621\tnan\n",
      "622\tnan\n",
      "623\tnan\n",
      "624\tnan\n",
      "625\tnan\n",
      "626\tnan\n",
      "627\tnan\n",
      "628\tnan\n",
      "629\tnan\n",
      "630\tnan\n",
      "631\tnan\n",
      "632\tnan\n",
      "633\tnan\n",
      "634\tnan\n",
      "635\tnan\n",
      "636\tnan\n",
      "637\tnan\n",
      "638\tnan\n",
      "639\tnan\n",
      "640\tnan\n",
      "641\tnan\n",
      "642\tnan\n",
      "643\tnan\n",
      "644\tnan\n",
      "645\tnan\n",
      "646\tnan\n",
      "647\tnan\n",
      "648\tnan\n",
      "649\tnan\n",
      "650\tnan\n",
      "651\tnan\n",
      "652\tnan\n",
      "653\tnan\n",
      "654\tnan\n",
      "655\tnan\n",
      "656\tnan\n",
      "657\tnan\n",
      "658\tnan\n",
      "659\tnan\n",
      "660\tnan\n",
      "661\tnan\n",
      "662\tnan\n",
      "663\tnan\n",
      "664\tnan\n",
      "665\tnan\n",
      "666\tnan\n",
      "667\tnan\n",
      "668\tnan\n",
      "669\tnan\n",
      "670\tnan\n",
      "671\tnan\n",
      "672\tnan\n",
      "673\tnan\n",
      "674\tnan\n",
      "675\tnan\n",
      "676\tnan\n",
      "677\tnan\n",
      "678\tnan\n",
      "679\tnan\n",
      "680\tnan\n",
      "681\tnan\n",
      "682\tnan\n",
      "683\tnan\n",
      "684\tnan\n",
      "685\tnan\n",
      "686\tnan\n",
      "687\tnan\n",
      "688\tnan\n",
      "689\tnan\n",
      "690\tnan\n",
      "691\tnan\n",
      "692\tnan\n",
      "693\tnan\n",
      "694\tnan\n",
      "695\tnan\n",
      "696\tnan\n",
      "697\tnan\n",
      "698\tnan\n",
      "699\tnan\n",
      "700\tnan\n",
      "701\tnan\n",
      "702\tnan\n",
      "703\tnan\n",
      "704\tnan\n",
      "705\tnan\n",
      "706\tnan\n",
      "707\tnan\n",
      "708\tnan\n",
      "709\tnan\n",
      "710\tnan\n",
      "711\tnan\n",
      "712\tnan\n",
      "713\tnan\n",
      "714\tnan\n",
      "715\tnan\n",
      "716\tnan\n",
      "717\tnan\n",
      "718\tnan\n",
      "719\tnan\n",
      "720\tnan\n",
      "721\tnan\n",
      "722\tnan\n",
      "723\tnan\n",
      "724\tnan\n",
      "725\tnan\n",
      "726\tnan\n",
      "727\tnan\n",
      "728\tnan\n",
      "729\tnan\n",
      "730\tnan\n",
      "731\tnan\n",
      "732\tnan\n",
      "733\tnan\n",
      "734\tnan\n",
      "735\tnan\n",
      "736\tnan\n",
      "737\tnan\n",
      "738\tnan\n",
      "739\tnan\n",
      "740\tnan\n",
      "741\tnan\n",
      "742\tnan\n",
      "743\tnan\n",
      "744\tnan\n",
      "745\tnan\n",
      "746\tnan\n",
      "747\tnan\n",
      "748\tnan\n",
      "749\tnan\n",
      "750\tnan\n",
      "751\tnan\n",
      "752\tnan\n",
      "753\tnan\n",
      "754\tnan\n",
      "755\tnan\n",
      "756\tnan\n",
      "757\tnan\n",
      "758\tnan\n",
      "759\tnan\n",
      "760\tnan\n",
      "761\tnan\n",
      "762\tnan\n",
      "763\tnan\n",
      "764\tnan\n",
      "765\tnan\n",
      "766\tnan\n",
      "767\tnan\n",
      "768\tnan\n",
      "769\tnan\n",
      "770\tnan\n",
      "771\tnan\n",
      "772\tnan\n",
      "773\tnan\n",
      "774\tnan\n",
      "775\tnan\n",
      "776\tnan\n",
      "777\tnan\n",
      "778\tnan\n",
      "779\tnan\n",
      "780\tnan\n",
      "781\tnan\n",
      "782\tnan\n",
      "783\tnan\n",
      "784\tnan\n",
      "785\tnan\n",
      "786\tnan\n",
      "787\tnan\n",
      "788\tnan\n",
      "789\tnan\n",
      "790\tnan\n",
      "791\tnan\n",
      "792\tnan\n",
      "793\tnan\n",
      "794\tnan\n",
      "795\tnan\n",
      "796\tnan\n",
      "797\tnan\n",
      "798\tnan\n",
      "799\tnan\n",
      "800\tnan\n",
      "801\tnan\n",
      "802\tnan\n",
      "803\tnan\n",
      "804\tnan\n",
      "805\tnan\n",
      "806\tnan\n",
      "807\tnan\n",
      "808\tnan\n",
      "809\tnan\n",
      "810\tnan\n",
      "811\tnan\n",
      "812\tnan\n",
      "813\tnan\n",
      "814\tnan\n",
      "815\tnan\n",
      "816\tnan\n",
      "817\tnan\n",
      "818\tnan\n",
      "819\tnan\n",
      "820\tnan\n",
      "821\tnan\n",
      "822\tnan\n",
      "823\tnan\n",
      "824\tnan\n",
      "825\tnan\n",
      "826\tnan\n",
      "827\tnan\n",
      "828\tnan\n",
      "829\tnan\n",
      "830\tnan\n",
      "831\tnan\n",
      "832\tnan\n",
      "833\tnan\n",
      "834\tnan\n",
      "835\tnan\n",
      "836\tnan\n",
      "837\tnan\n",
      "838\tnan\n",
      "839\tnan\n",
      "840\tnan\n",
      "841\tnan\n",
      "842\tnan\n",
      "843\tnan\n",
      "844\tnan\n",
      "845\tnan\n",
      "846\tnan\n",
      "847\tnan\n",
      "848\tnan\n",
      "849\tnan\n",
      "850\tnan\n",
      "851\tnan\n",
      "852\tnan\n",
      "853\tnan\n",
      "854\tnan\n",
      "855\tnan\n",
      "856\tnan\n",
      "857\tnan\n",
      "858\tnan\n",
      "859\tnan\n",
      "860\tnan\n",
      "861\tnan\n",
      "862\tnan\n",
      "863\tnan\n",
      "864\tnan\n",
      "865\tnan\n",
      "866\tnan\n",
      "867\tnan\n",
      "868\tnan\n",
      "869\tnan\n",
      "870\tnan\n",
      "871\tnan\n",
      "872\tnan\n",
      "873\tnan\n",
      "874\tnan\n",
      "875\tnan\n",
      "876\tnan\n",
      "877\tnan\n",
      "878\tnan\n",
      "879\tnan\n",
      "880\tnan\n",
      "881\tnan\n",
      "882\tnan\n",
      "883\tnan\n",
      "884\tnan\n",
      "885\tnan\n",
      "886\tnan\n",
      "887\tnan\n",
      "888\tnan\n",
      "889\tnan\n",
      "890\tnan\n",
      "891\tnan\n",
      "892\tnan\n",
      "893\tnan\n",
      "894\tnan\n",
      "895\tnan\n",
      "896\tnan\n",
      "897\tnan\n",
      "898\tnan\n",
      "899\tnan\n",
      "900\tnan\n",
      "901\tnan\n",
      "902\tnan\n",
      "903\tnan\n",
      "904\tnan\n",
      "905\tnan\n",
      "906\tnan\n",
      "907\tnan\n",
      "908\tnan\n",
      "909\tnan\n",
      "910\tnan\n",
      "911\tnan\n",
      "912\tnan\n",
      "913\tnan\n",
      "914\tnan\n",
      "915\tnan\n",
      "916\tnan\n",
      "917\tnan\n",
      "918\tnan\n",
      "919\tnan\n",
      "920\tnan\n",
      "921\tnan\n",
      "922\tnan\n",
      "923\tnan\n",
      "924\tnan\n",
      "925\tnan\n",
      "926\tnan\n",
      "927\tnan\n",
      "928\tnan\n",
      "929\tnan\n",
      "930\tnan\n",
      "931\tnan\n",
      "932\tnan\n",
      "933\tnan\n",
      "934\tnan\n",
      "935\tnan\n",
      "936\tnan\n",
      "937\tnan\n",
      "938\tnan\n",
      "939\tnan\n",
      "940\tnan\n",
      "941\tnan\n",
      "942\tnan\n",
      "943\tnan\n",
      "944\tnan\n",
      "945\tnan\n",
      "946\tnan\n",
      "947\tnan\n",
      "948\tnan\n",
      "949\tnan\n",
      "950\tnan\n",
      "951\tnan\n",
      "952\tnan\n",
      "953\tnan\n",
      "954\tnan\n",
      "955\tnan\n",
      "956\tnan\n",
      "957\tnan\n",
      "958\tnan\n",
      "959\tnan\n",
      "960\tnan\n",
      "961\tnan\n",
      "962\tnan\n",
      "963\tnan\n",
      "964\tnan\n",
      "965\tnan\n",
      "966\tnan\n",
      "967\tnan\n",
      "968\tnan\n",
      "969\tnan\n",
      "970\tnan\n",
      "971\tnan\n",
      "972\tnan\n",
      "973\tnan\n",
      "974\tnan\n",
      "975\tnan\n",
      "976\tnan\n",
      "977\tnan\n",
      "978\tnan\n",
      "979\tnan\n",
      "980\tnan\n",
      "981\tnan\n",
      "982\tnan\n",
      "983\tnan\n",
      "984\tnan\n",
      "985\tnan\n",
      "986\tnan\n",
      "987\tnan\n",
      "988\tnan\n",
      "989\tnan\n",
      "990\tnan\n",
      "991\tnan\n",
      "992\tnan\n",
      "993\tnan\n",
      "994\tnan\n",
      "995\tnan\n",
      "996\tnan\n",
      "997\tnan\n",
      "998\tnan\n",
      "999\tnan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38680/1549685513.py:49: RuntimeWarning: overflow encountered in matmul\n",
      "  self.pre_activation = np.matmul(x, self.w.T) + self.b\n",
      "/tmp/ipykernel_38680/1549685513.py:49: RuntimeWarning: invalid value encountered in matmul\n",
      "  self.pre_activation = np.matmul(x, self.w.T) + self.b\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'epoch': [],\n",
    "    'loss': []\n",
    "}\n",
    "\n",
    "N = 3\n",
    "\n",
    "for i in range(1000):\n",
    "    model.zero_grad()\n",
    "    y_pred = model.forward(X_train_small_scaled[:N])\n",
    "    loss = loss_fn.forward(y_pred, Y_train_small[:N])\n",
    "    loss_grad = loss_fn.backward(y_pred, Y_train_small[:N])\n",
    "    model.backward(loss_grad, Y_train_small[:N])\n",
    "    optimizer.step()\n",
    "    print(i, loss, sep='\\t')\n",
    "    history['epoch'].append(i)\n",
    "    history['loss'].append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['epoch'], history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
